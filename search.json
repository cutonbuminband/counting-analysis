[
  {
    "objectID": "time.html",
    "href": "time.html",
    "title": "Tick tock: looking at counting activity over time",
    "section": "",
    "text": "Reddit helpfully provides us with time stamps for each comment, so we know exactly when each count on the counting chain was made. This information can be used for a wealth of different things, but in this post I’ll focus on what the data can tell us about the daily rhythm of r/counting; about what time of day the subreddit is most active, and about how that has changed throughout the years.\nWe’ll start off with some code to import the relevant packages and load the data\nThe first thing we’ll look at is how the average rate of counts varies throughout the day. That’s shown on Figure 1.\nYou can see that the counting rate varies quite a bit - the most popular time (early afternoon EST) has an average counting rate that’s almost four times higher than the least popular time (late night/early morning EST). It’s also nice to have a feeling for what a “normal” rate of counting has been, and it turns that it’s on the order of one count per minute.\nIt’s of course also possible to calculate this variation for individual counters, and not just for all counters together. The counters in the top 30 with most and least variation are shown on Table 1.\nAs an aside, you should notice that even the most regular counter has a variation that is higher than the overall variation. Intuitively that makes sense - it’s harder for one person to evenly cover all 24 hours of the day than it is for all counters together. We can focus on the single least and most regular counters and look at how their counting rates have varied throughout the day. Here’s a plot of that:\nIt’s not too surprising, but those distributions look very different!"
  },
  {
    "objectID": "time.html#summary-statistics-and-the-circular-mean",
    "href": "time.html#summary-statistics-and-the-circular-mean",
    "title": "Tick tock: looking at counting activity over time",
    "section": "Summary statistics and the circular mean",
    "text": "Summary statistics and the circular mean\nCreating and looking at the hexbin plot let us confirm the fact that just showing the average distribution hides a lot of structure. In particular, we can see that\n\nThe counting rate varies a lot over time\nThe time of day distribution also changes a lot\n\nQuantitatively, it’s difficult to say more than that based on the figure. There’s just too much going on, and it would be nice if we could simplify it.\nWhat we’d really need is some kind of summary statistic for a time of day distribution, because then we can easily plot how that summary statistic varies over time. An obvious first choice could be the mean of the distribution, to represent what time of day the average count takes place.\nUnfortunately, it’s not so simple. The time of day data is circular, and the standard mean is badly suited for this use case. To illustrate, we can consider what the average time is of two events, one occurring at 23:59 and the other at 00:001. If we just use the linear mean, we arrive at 12:00, but intuitively the answer should be 00:00.\nWhat we can use instead is the circular mean. You can imagine this as pretending we have a 24h analog clock, and each event is an arrow points to its correct time. The arrow tail is at (0, 0), and the arrow head is at position (x, y), corresponding to whatever time it is. What we want to do is to find the average angle of all the arrows, and to do that we average all the x positions separately, and all the y positions separately, and create a new arrow that points to (average x, average y). The angle we want is then the angle of this arrow.\nWe can do that for the overall counting distribution to obtain\n\n\nCode\nmean = scipy.stats.circmean(counts['time'], high=units.DAY)\nhour, rem = divmod(mean, 3600)\nminute, second = divmod(rem, 60)\nprint(f\"The mean of the overall distribution is {int(hour):02d}:{int(minute):02d}\")\n\n\nThe mean of the overall distribution is 16:55\n\n\nThat seems reasonable - it’s inside the broad afternoon peak of activity, but slightly to the right, since there’s more activity in the evening than in the early morning.\nWith the summary statistic in hand, we can plot how the mean time of day of counts has varied over time\n\n\nCode to plot the average counting time over the years\ncounts[\"x\"] = np.cos(counts[\"time\"] / units.DAY * 2 * np.pi)\ncounts[\"y\"] = np.sin(counts[\"time\"] / units.DAY * 2 * np.pi)\nrolling = counts[[\"x\", \"y\", \"date\"]].rolling(\"28d\", on=\"date\").mean()\nrolling[\"time_of_day\"] = (\n    np.arctan2(rolling[\"y\"], rolling[\"x\"]) * units.DAY / 2 / np.pi\n) % units.DAY\nrolling = rolling.resample(\"7d\", on=\"date\").mean()\nrolling[\"tod\"] = pd.to_datetime(rolling[\"time_of_day\"], unit=\"s\")\nlabels = {\"date\": \"Date\", \"tod\": \"Time of Day (UTC-5)\"}\nfig = px.scatter(data_frame=rolling.reset_index(), x=\"date\", y=\"tod\", labels=labels)\n\nrolling[\"numerical_date\"] = mdates.date2num(rolling.index)\nmodel = np.poly1d(np.polyfit(rolling[\"numerical_date\"], rolling[\"time_of_day\"], 1))\nrestricted = rolling.iloc[[0, -1], :]\nfig.add_trace(\n    go.Scatter(\n        x=pd.to_datetime(restricted.index),\n        y=pd.to_datetime(model(restricted[\"numerical_date\"]), unit=\"s\"),\n        mode=\"lines\",\n        line_color=\"hsl(0, 0%, 30%)\",\n    )\n)\nfig.update_yaxes(autorange=\"reversed\", tickformat=\"%H:%M\")\nfig.update_layout(showlegend=False)\nfig.show()\n\n\n\n                                                \n\n\nThis analysis shows that from the start of r/counting until 2023, the average time of day of each count has drifted by about six hours. More precisely, we can say that\n\n\nCode\nMarkdown(f\"the average time has shifted by {np.polyfit(rolling['numerical_date'], rolling['time_of_day'], 1)[0]:.1f} seconds per day.\")\n\n\nthe average time has shifted by -5.4 seconds per day.\n\n\nThis shift is not something that was at all apparent from Figure 2, which shows the value of the summary statistic for revealing trends in the data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my website for publishing interesting bits and pieces of r/counting data and graphs in a central place. The code builds on the rcounting tools that I develop here"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "Imports and initialization\nWe start with some imports, which aren’t particularly interesting, so they’ve been folded.\n\n\nCode for importing packages\nimport pandas as pd\nfrom rcounting import side_threads, thread_navigation as tn\n\n\n\n\nValidating side threads\nThe rcounting tools have various pieces of functionality to validate the side threads it knows about, both in terms of whether the counts follow the expected format, and whether the thread obeys any special rules that might apply. Here’s an example of what that looks like in a python script.\n\n\nCode\n# Pick a comment early in a chain\ncomments = pd.DataFrame(tn.fetch_comments(\"gja0ehe\"))\nside_thread = side_threads.get_side_thread('slowestest')\nprint(\"The thread is valid!\" if side_thread.is_valid_thread(comments) else \"The thread is invalid\")\n\n\nThe thread is valid!\n\n\nThe thread is valid - excellent! There’s also a script that you can run from the command line to validate the most common threads. Try typing rcounting validate -h in a terminal to see how to use it.\n\n\nNetwork analysis\nThe rcounting tools also have functionality to do some network analysis. The following snippet will generate the (comment, replying to, weight) graph for the top 250 counters. The heavy lifting is done by the response graph [[file:../rcounting/analysis.py::def response_graph(df, n=250, username_column=“username”):][response_graph]] function in analysis.py.\n\n\nCode\nfrom rcounting import analysis, counters\nimport sqlite3\nfrom pathlib import Path\ndata_directory = Path(\"../data\")\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\ncounts = pd.read_sql(\"select comments.username \"\n                       \"from comments join submissions \"\n                       \"on comments.submission_id = submissions.submission_id \"\n                       \"where comments.position > 0 \"\n                       \"order by submissions.timestamp, comments.position\", db)\ncounts[\"username\"] = counts[\"username\"].apply(counters.apply_alias)\nn = 250\ngraph = analysis.response_graph(counts, n, username_column=\"username\")\ngraph.sort_values(ascending=False, by=\"Weight\").head()\n\n\n\n\n\n\n  \n    \n      \n      Source\n      Target\n      Weight\n    \n  \n  \n    \n      14960\n      thephilsblogbar2\n      GarlicoinAccount\n      98844\n    \n    \n      2869\n      GarlicoinAccount\n      thephilsblogbar2\n      98821\n    \n    \n      1657\n      Countletics\n      nonsensy\n      92922\n    \n    \n      12832\n      nonsensy\n      Countletics\n      92922\n    \n    \n      873\n      Antichess\n      Countletics\n      73007"
  },
  {
    "objectID": "separators.html",
    "href": "separators.html",
    "title": "The use of separators",
    "section": "",
    "text": "We have access to the body of each comment, so it’s possible to do some of analysis on those. One interesting thing could be to look at whether a given count is comma separated, space separated or uses no separator at all. And a natural question to ask is how the distribution between those three types has changed over time\nSpecifically, we’ll define the three types of count as:\n\nComma separated counts look like [digit]*{1-3}(,[digit]*3)*\nSpace separated counts are the same, with the comma replaced by a space\nNo separated counts are defined as one of\n\nCounts with only one digit\nCounts with no separators between their first and last digit, with separators defined fairly broadly.\n\n\n\n\nCode for importing packages and loading data\nimport re\nimport sqlite3\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport seaborn as sns\nfrom rcounting import analysis, counters, parsing, side_threads\nfrom rcounting import thread_navigation as tn\nfrom rcounting.reddit_interface import reddit\n\npio.templates.default = \"seaborn\"\nsns.set_theme()\nfrom IPython.display import Markdown\n\ndata_directory = Path(\"../data\")\n\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\n\ncounts = pd.read_sql(\n    \"select comments.body, comments.timestamp from comments join submissions \"\n    \"on comments.submission_id = submissions.submission_id where comments.position > 0 \"\n    \"order by submissions.timestamp, comments.position\",\n    db,\n)\ncounts[\"date\"] = pd.to_datetime(counts[\"timestamp\"], unit=\"s\")\ncounts.drop(\"timestamp\", inplace=True, axis=1)\n\n\nWe started by making the necessary imports and loading all the data; with that out of the way we can implement the rules defined above\n\n\nCode\ndata = counts.set_index(\"date\")\n\ndata[\"body\"] = data[\"body\"].apply(parsing.strip_markdown_links)\ncomma_regex = re.compile(r\"\\d{1,3}(?:,\\d{3})+\")\ndata[\"commas\"] = data[\"body\"].apply(lambda x: bool(re.search(comma_regex, x)))\nspace_regex = re.compile(r\"\\d{1,3}(?: \\d{3})+\")\ndata[\"spaces\"] = data[\"body\"].apply(lambda x: bool(re.search(space_regex, x)))\n\n\ndef no_separators(body):\n    body = body.split(\"\\n\")[0]\n    separators = re.escape(\"' , .*/\")\n    regex = rf\"(?:^[^\\d]*\\d[^\\d]*$)|\" rf\"(?:^[^\\d]*\\d[^{separators}]*\\d[^\\d]*$)\"\n    regex = re.compile(regex)\n    result = re.search(regex, body)\n    return bool(result)\n\n\ndata[\"no separator\"] = data[\"body\"].apply(no_separators)\ndata.sort_index(inplace=True)\n\n\nOnce we have the data, we can get a 14-day rolling average, and resample the points to nice 6h intervals. The resampling makes plotting with pandas look nicer, since it can more easily deal with the x-axis.\n\n\nCode for plotting the separator data\nresampled = (\n    (data[[\"commas\", \"spaces\", \"no separator\"]].rolling(\"14d\").mean() * 100)\n    .resample(\"6h\")\n    .mean()\n    .melt(ignore_index=False)\n    .reset_index()\n)\n\nlabels = {\n    \"date\": \"Date\",\n    \"variable\": \"Separator style\",\n    \"value\": \"Percentage of counts\",\n}\nfig = px.line(\n    data_frame=resampled,\n    x=\"date\",\n    y=\"value\",\n    color=\"variable\",\n    labels=labels,\n    title=\"The separators used on r/counting over time\"\n)\n\nfig.update_yaxes(range=[0, 100])\nfig.show()\n\n\n\n                                                \n\n\nNotice you can clearly see when the count crossed 100k: that’s when the ‘no separators’ line quickly drops from being the majority to being a clear minority of counts. That was followed by the era of commas, when the default format was just to use commas as separators. Over the last years, commas have significantly declined, and have now been overtaken by spaces as the most popular separator, although there’s a lot of variation depending on who exactly is active. No separators has bouts of activity, but is generally below the other two options. Pretty neat!"
  },
  {
    "objectID": "runs.html",
    "href": "runs.html",
    "title": "Longest runs",
    "section": "",
    "text": "Most counts on r/counting are made by two counters collaborating; the signature of this is that the n^th count and the (n + 2)^th count almost always have the same author. Usually we only consider such collaborative counting a run when the two counters reply to each other at a fairly rapid clip, but I’ve ignored that here\nFirstly, I’ve looked at the longest runs by total time, which I’ve defined as the longest periods of time when only two people counted in main. Unfortunately, that doesn’t give anything very interesting: these are all from early in the subreddit history, where hours would frequently pass between replies, and the runs I’ve found are generally less than ten counts long. An example is from the 24k counting thread, where just over 23 hours passed between two counts here here1. There was only one exception to this trend in the 2M era. Apparently we had a bit of a problem with spammers & farmers back then, and a really long chain of counts was deleted, and the count was continued from a valid point some hours later. The whole thing caused a bit of confusion.1 Plus that was a late chain which somehow became official. I guess we were less strict back then. \n\n\nCode\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport re\nimport sqlite3\nimport matplotlib.pyplot as plt\n\nfrom rcounting import side_threads, counters, analysis, thread_navigation as tn, parsing, units\nfrom rcounting.reddit_interface import reddit\nimport seaborn as sns\nsns.set_theme()\nfrom IPython.display import Markdown\ndata_directory = Path(\"../data/\")\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\ndf = pd.read_sql(\"select counters.canonical_username as username, timestamp, comment_id from comments join counters on comments.username=counters.username where comments.position > 0 order by timestamp\", db)\n\n\nLooking at the longest runs by total number of counts in the run is more interesting. so let’s do that. We take all our ordered counts and shift them by two; if the two authors are different, that means one streak has ended and a new one started. Taking the cumulative sum of all these changes means that each run is assigned the same number; exactly what we want. Table 1 shows the top ten runs in r/counting history, as well as their total length.2:2 I haven’t checked through all of them, but it’s likely all the lengths are off by one. That’s because I’ve forced every comment to be part of only one run, but the first comment in each run should simultaneously be the last comment in the previous run.\n\n\nCode\ncolumn = df['username']\nis_different_group = (column != column.shift(2))\ndf['group'] = is_different_group.cumsum()\ngroups = df.groupby('group')\nindices = (-groups.size().loc[groups.size() > 1]).sort_values(kind='mergesort').index\nold = groups.first().loc[indices]\nnew = groups.last().loc[indices]\nnew['username'] = groups.nth(1).loc[indices]['username']\nold['length'] = groups.size().loc[indices]\ncombined = old.join(new, rsuffix='2', lsuffix='1')\ncombined['dt'] = (combined['timestamp2'] - combined['timestamp1']) / units.HOUR\ncombined.drop(['timestamp1', 'timestamp2'], axis=1, inplace=True)\n\ndef link(comment):\n    body, submission = pd.read_sql(f\"select body, comment_id from comments where comment_id == '{comment}' order by timestamp desc limit 1\", db).iloc[0]\n    return f\"[{parsing.find_count_in_text(body):,}](/comments/{submission}/_/{comment})\"\n\nfinal = combined.head(10).reset_index(drop=True).copy()\n\nfinal[\"old_link\"] = final[\"comment_id1\"].apply(link)\nfinal[\"new_link\"] = final[\"comment_id2\"].apply(link)\nfinal.index += 1\ndef format_time(timedelta):\n    hours, rem = divmod(timedelta.total_seconds(), 3600)\n    minutes, seconds = divmod(rem, 60)\n    return f\"{int(hours):0>2}:{int(minutes):0>2}\"\nfinal['dt'] = pd.to_timedelta(final['dt'], unit='h').round('s').apply(format_time)\nMarkdown(final[['username1', 'username2', 'old_link', 'new_link', 'length', 'dt']].to_markdown(headers=['**Rank**', '**1st counter**', '**2nd counter**', '**Start**', '**End**', '**Length**', '**Duration**']))\n\n\n\n\nTable 1: The 10 longest runs in r/counting history\n\n\nRank\n1st counter\n2nd counter\nStart\nEnd\nLength\nDuration\n\n\n\n\n1\ndavidjl123\nCountletics\n3,037,002\n3,044,030\n7029\n02:44\n\n\n2\ndavidjl123\nCountletics\n3,190,001\n3,195,027\n5027\n02:11\n\n\n3\nCountletics\ndavidjl123\n3,101,028\n3,105,791\n4764\n01:54\n\n\n4\nCountletics\ndavidjl123\n3,072,012\n3,076,715\n4704\n01:56\n\n\n5\nCountletics\ndavidjl123\n3,201,005\n3,205,000\n3996\n01:41\n\n\n6\nCountletics\ndavidjl123\n3,090,222\n3,094,002\n3781\n01:45\n\n\n7\nCountletics\ndavidjl123\n3,064,318\n3,068,003\n3686\n01:37\n\n\n8\nCountletics\nLeMinerWithCheese\n4,569,377\n4,572,402\n3026\n02:43\n\n\n9\nAntichess\nCountletics\n4,218,058\n4,221,003\n2946\n01:31\n\n\n10\ndavidjl123\nGarlicoinAccount\n4,028,324\n4,031,002\n2679\n02:22\n\n\n\n\n\n\nThe first seven runs are by u/davidjl123 and u/Countletics in the early 3Ms, and all of the top ten involve either david or countletics. The first one which doesn’t either of them is number 17 between nonsensy and colby6666, starting at 3,456,003 and continuing for 2000 counts."
  },
  {
    "objectID": "counting_counters.html",
    "href": "counting_counters.html",
    "title": "Counting counters",
    "section": "",
    "text": "I’ve previously described r/counting as a collaborative incremental game, and that for me sums up the essence of counting fairly well. A natural question to ask about the game is how many people have played over the years\nWe’ll start of by importing the relevant packages and loading some data. Since we’re only interested in the counters in each thread, we only load those two columns from the database.\n\n\nCode for importing packages and loading data\nimport re\nimport sqlite3\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport scipy\nimport seaborn as sns\nfrom IPython.display import Markdown\nfrom rcounting import parsing\n\nsns.set_theme()\npio.templates.default = \"seaborn\"\ndata_directory = Path(\"../data\")\n\npd.options.plotting.backend = \"plotly\"\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\n\ncounts = pd.read_sql(\n    \"select counters.canonical_username as username, submission_id from comments \"\n    \" join counters on comments.username=counters.username \"\n    \"where comments.position > 0 and submission_id != 'uuikz' order by timestamp\",\n    db,\n)\nsubmissions = pd.read_sql(\"select * from submissions\", db)\n\n\ndef format_title(row):\n    return (\n        f\"[{row.title}](http://www.reddit.com/r/counting/comments/{row.submission_id})\"\n    )\n\n\nsubmissions[\"link\"] = submissions.apply(format_title, axis=1)\n\n\nNow finding the total number of counters is easy\n\n\nCode\ncounts[\"username\"].nunique()\n\n\n15690\n\n\nThat’s more than I was expecting!\n\nThe number of counters in each thread\nThe counts in r/counting are split into threads of 1000 counts each, and in principle it should be possible to have a thread with 1000 different counters participating. That’s never happened, especially since most counts are made as part of a series of replies between just two users. Still, it might be interesting to see which threads had the most counters taking part:\n\n\nCode\nlevels = counts.groupby(['submission_id', 'username'], sort=False).size()\ntop = levels.groupby(level=0, sort=False).size().sort_values(ascending=False).head()\ntop_submissions =  submissions.query(\"submission_id in @top.index\").copy()\ncombined = pd.concat([top, top_submissions.set_index(\"submission_id\")], axis=1)\nMarkdown(combined[[\"link\", 0]].to_markdown(headers=[\"**Thread**\", \"**Number of counters**\"], index=False))\n\n\n\n\n\nThread\nNumber of counters\n\n\n\n\n100k Counting thread. We are now in the land of six digits.\n189\n\n\n1,103k Counting Thread\n172\n\n\n250K Counting Thread\n148\n\n\n336K Counting Thread\n146\n\n\n389k Counting Thread\n144\n\n\n\n\n\nSome of these threads really had a lot of participants!\nOn the oppositve end of the scale, we can look at the threads with fewest participants. Since you’re not allowed to reply to yourself, at least two people have to take part in each thread. We can easily see how many times that’s happened:\n\n\nCode\nperfect = levels.groupby(level=0, sort=False).size() == 2\nperfect = perfect.loc[perfect].index\nlen(perfect)\n\n\n161\n\n\nSo not a huge amount of times, but it’s happened. The last five threads with only two counters are\n\n\nCode\nperfect_500s = submissions.query(\"submission_id in @perfect\").copy().tail().iloc[::-1]\ndef find_counters(submission_id):\n    return pd.Series(levels.loc[submission_id].index)\nperfect_500s[[\"first_counter\", \"second_counter\"]] = perfect_500s[\"submission_id\"].apply(find_counters)\nMarkdown(perfect_500s[[\"link\", \"first_counter\", \"second_counter\"]].to_markdown(headers=[\"**Thread**\", \"**First Counter**\", \"**Second Counter**\"], index=False))\n\n\n\n\n\nThread\nFirst Counter\nSecond Counter\n\n\n\n\n5078k Counting Thread\nAntichess\nCountletics\n\n\n5079k Counting Thread\nAntichess\nCountletics\n\n\n5,104k Counting Thread\nClockButTakeOutTheL\nAntichess\n\n\n5120k Counting Thread\nAntichess\nCountletics\n\n\n5121k Counting Thread\nAntichess\nCountletics\n\n\n\n\n\nWe can plot the distribution of the number of counters in each thread; this is shown on Figure 1.\n\n\nCode\ncounters = levels.groupby(level=0, sort=False).size()\nfig = px.histogram(\n    list(counters[counters <= 100]),\n    labels={\"value\": \"Number of Counters\"},\n)\nfig.update_layout(showlegend=False, yaxis_title_text='Occurences')\nfig.show()\n\n\n\n\n                                                \nFigure 1: The distribution of the number of counters participating in a thread\n\n\n\n\n\nEffective number of counters per thread\nThe total number of counters that participate in a thread is an inherently noisy quantity. One person making a single count can change the total even if they make no other counts in the thread. A better way is to look at the effective number of counters taking part in a thread. The effective number takes into account how skewed the distribution of participants is. If 10 people count 100 times each in a thread, then both the actual and the effective number of counters is 10. If instead two people count 496 times each, and 8 people count once each, then the effective number of counters is 2.02, because two people made basically all the counts.\nWe can find the submission with the highest number of effective counters.\n\n\nCode\nfrom rcounting.analysis import effective_number_of_counters\neffective_counters = levels.groupby(level=0, sort=False).apply(effective_number_of_counters)\nsubmission_id = effective_counters.idxmax()\ns = (f\"The thread with the highest number of effective counters is \"\n     f\"{submissions.query('submission_id == @submission_id')['link'].iat[0]}, \"\n     f\"with {effective_counters.loc[submission_id]:.1f} counters.\")\nMarkdown(s)\n\n\nThe thread with the highest number of effective counters is 336K Counting Thread, with 28.2 counters.\n\n\nWe can also compare the total and the effective number of counters\n\n\nCode\ntotal_counters = levels.groupby(level=0, sort=False).size()\nmerged = (pd.concat([effective_counters, total_counters], axis=1))\nmerged.columns = ['Effective counters', 'Actual counters']\n\n\n\n\nCode\ntable = merged.describe().transpose()[[\"mean\", \"50%\", \"max\"]]\nMarkdown(table.to_markdown(floatfmt=\".1f\", headers=[\"**Mean**\", \"**Median**\", \"**Maximum**\"]))\n\n\n\n\n\n\nMean\nMedian\nMaximum\n\n\n\n\nEffective counters\n4.5\n3.5\n28.2\n\n\nActual counters\n20.5\n18.0\n189.0\n\n\n\n\n\nWe can see that both the total and effective number of counters have a median that is lower than the mean, indicating that the distributions have long tails to the right. We can plot these, which is done on figure Figure 2. You can clearly see how much more spread out the actual number of counters is compared with the effective number. The effective number is really sharply peaked at 2, with 25% of the counts lying in the range 2-2.4.\n\n\nCode\nlimits = [0, 50]\nkde1 = scipy.stats.gaussian_kde(merged[\"Effective counters\"])\nkde2 = scipy.stats.gaussian_kde(merged[\"Actual counters\"])\naxis = np.linspace(*limits, 100, endpoint=False)\ndata = pd.DataFrame(\n    {\n        \"Number of counters\": axis,\n        \"Effective counters\": kde1(axis),\n        \"Actual counters\": kde2(axis),\n    }\n)\n\nfig = px.line(\n    data_frame=data.melt(id_vars=[\"Number of counters\"]),\n    x=\"Number of counters\",\n    y=\"value\",\n    color=\"variable\",\n    labels={\"value\": \"Probability density\", \"variable\": \"Model\"},\n)\nfig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99))\nfig.update_yaxes(range=(0, 0.28))\n\nfig.show()\n\n\n\n\n                                                \nFigure 2: The distributions of the number of effective and actual counters in each thread\n\n\n\nWe can also plot how the effective and actual number of counters have evolved throughout r/counting history; this is shown on figure Figure 3. The actual and effective number of counters track each other quite closely across threads. It seems there’s been a gradual decline in the number of counters participating in each thread, but with spikes of activity. One thing I was expecting to see was clear spikes at 100k threads, since running isn’t allowed on those. And those spikes just aren’t apparent in the data.\n\n\n\n\n                                                \nFigure 3: How the number of effective and actual counters has changed through r/counting history, a 10-thread rolling average\n\n\n\nWe can also plot the effective number of counters as a function of the actual number of counters. You can see generally, the more actual counters there are ina thread, there more effective counters there will be, but the relationship is fairly noisy.\n\n\nCode\nfig = px.scatter(data_frame=merged, x=\"Actual counters\", y=\"Effective counters\", trendline=\"ols\")\nfig.update_traces(opacity=0.5)\nfig.update_yaxes(range=(2, 25))\nfig.update_xaxes(range=(0, 150))\nfig.show()"
  },
  {
    "objectID": "network.html",
    "href": "network.html",
    "title": "The r/counting network",
    "section": "",
    "text": "One of the interesting things we can look at using the counting data is the relationships between different counters. For example, and as an introduction, we can ask which counters have replied to each other most often\nTable 1 shows the 10 pairs of counters who have replied directly to each other most often. Evidently it’s more likely that someone will appear in the table if they’ve made a lot of counts, but it’s still interesting to see that the top spot isn’t held by the top two counters.\nWe can also see which counters have counted with the most other people."
  },
  {
    "objectID": "network.html#the-core-of-the-rcounting-graph",
    "href": "network.html#the-core-of-the-rcounting-graph",
    "title": "The r/counting network",
    "section": "The core of the r/counting graph",
    "text": "The core of the r/counting graph\nThe counting community has evolved over time, with new people dropping in, and older counters fading away (and sometimes staging remarkable comebacks).\nIn the counting graph, one person is connected to another if they’ve ever replied to each another. The degree of a person is a count of how many connections they have. There’s a really neat approach to finding the most connected group of people in the graph that goes as follows:\n\nDefine the connectivity score of the graph as the degree of the least-connected person in the graph\nRemove the least-connected person in the graph and see what happens to the connectivity score\nKeep going until the connectivity score starts to decrease.\n\nWhen you remove one person, you do to things that might affect the overall connectivity score:\n\nYou remove the least-connected person, so in everyone that remains is at least as well connected as that person, and possibly more connected\nYou decrease the degree of everyone that was directly connected to the least-connected person, possibly causing the overall connectivity score to decrease.\n\nDoing that for the counting graph we get\n\n\nCode\nfrom networkx.algorithms.core import core_number\ncore = core_number(G)\nmax_core = max(core.values())\nunweighted_core = [key for key in core.keys() if core[key] == max(core.values())]\nMarkdown(f\"There are {len(unweighted_core)} counters in the unweighted core.\")\n\n\nThere are 118 counters in the unweighted core.\n\n\n\nThe weighted core\nThe approach I’ve just described has an important flaw in that it completely ignores how often two counters have interacted, and only looks at whether they are connected. That means that the connection between two counters who have only one count together is given the same importance as the connection between the counters in Table 1. That seems unfortunate.\nOne way of proceeding would be to apply a threshold and only link two counters in the graph if they have counted together more than X times. That gets rid of the “One count is equivalent to arbitrarily many counts” issue, but isn’t very satisfactory - instead, we get “X - 1 counts is equivalent to 0”, and “X counts is equivalent to arbitrarily many”.\nA better way would be if the strength of the connection could be incorporated into the calculation of the core. I’ll spare you the details, but doing so is a bit tricky. When the network is unweighted, there is a fast algorithm for finding the core (Batagelj and Zaversnik 2003), but adding weights breaks that algorithm. I ended up implementing it myself, you can see the implementation here if you want.\n\nBatagelj, V., and M. Zaversnik. 2003. “An o(m) Algorithm for Cores Decomposition of Networks.” https://arxiv.org/abs/cs/0310049.\nOnce I have a method for taking into account the weighted degree of each node, there are two questions to consider:\n\nHow to model the strength of a single connection\nHow to model the total weight of a node, based on the strength of all the connections it has with other nodes\n\nThe first question is absolutely vital to ask. If the strength of a connection between two counters is defined as just the total number of counts they have together, then no matter what else I do, the core ends up consisting of very few people who all have a lot of counts together.\n\n\nCode\ncoreness = graph_tools.weighted_core_number(G, p=1)\nmax_core_value = max(coreness.values())\nunscaled_core = [x for x in core if coreness[x] == max_core_value]\ns = f\"There are {len(unscaled_core)} counters in the core. They are:\\n\\n - \" + \"\\n- \".join(unscaled_core)\nMarkdown(s)\n\n\nThere are 4 counters in the core. They are:\n\nCountletics\nnonsensy\nthephilsblogbar2\nGarlicoinAccount\n\n\n\nA choice that works fairly well is to model the strength of the connection as the logarithm of the total number of counts. That lets more intense connections have more importance, but within reason.\nThe second question is a bit more subtle, since there’s an intuitive choice that works fairly well, namely just using the sum of all the connection strengths. But that’s not the only way to do things. In the end I ended up taking a weighted combination of the degree of the node and the total connection strength, so that the weighted degree of node \\(i\\), \\(k'_{i}\\) is given by\n\\[\nk'_{i}= \\left(k_{i}\\right)^{1 - p} \\left(\\sum _{\\textrm{neighbors} j}{w_{ij}}\\right)^{p}\n\\]\nwhere the sum runs over all neighbors \\(j\\) of node \\(i\\), \\(w_{ij}\\) is the strength of the connection between \\(i\\) and \\(j\\), and \\(p\\) is a parameter I choose that varies between \\(0\\) and \\(1\\). Setting \\(p = 0\\) means that only the unweighted degree of the node is considered, while setting \\(p = 1\\) means that only the sum of connection strengths matters. In between, you get a mix.\n\n\nCode\ngraph_tools.scale_weights(G)\ncoreness = graph_tools.weighted_core_number(G, p=1)\nmax_core_value = max(coreness.values())\nweighted_core = [x for x in core if coreness[x] == max_core_value]\nnx.set_node_attributes(G, \"periphery\", name=\"k-core\")\nnx.set_node_attributes(G.subgraph(weighted_core), \"core\", name=\"k-core\")\nnx.write_gexf(G, \"../data/graph.gexf\")\ns = f\"There are {len(weighted_core)} counters in the weighted core.\"\nMarkdown(s)\n\n\nThere are 93 counters in the weighted core.\n\n\nThis is a slightly smaller number then counters who were in the core for the unweighted case, and there’s also some difference in the composition of the members that remain:\n\n\nCode\nMarkdown(f\"There are {len(set(weighted_core) ^ set(unweighted_core))} present in only one of the weighted or unweighted core.\")\n\n\nThere are 29 present in only one of the weighted or unweighted core.\n\n\nWith the core in hand, it’s possible to visualise the counting graph again, this time highlighting the members of the weighted core, as shown on Figure 3\n\n\n\nFigure 3: The ~100 core members of the counting graph highlighted in green\n\n\nInterestingly enough the core mainly seems to correspond to the blue team shown on Figure 1, so perhaps my earlier suggestion that the colours mainly correspond to age is incorrect."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysing rcounting data",
    "section": "",
    "text": "This page highlights some of the initial analysis I’ve done of the counts made on r/counting. To do this, I use the database of all counts that was put together by u/davidjl123 and u/Countletics (and others!), as modified by me. For more niche analysis, see some of the other pages on the sidebar on the left. Most of the figures and tables here have also been posted on the subreddit, but I wanted to have them in a central place. I also liked being able to show the code and the tables or figures it generates in the same document, so that people can see both. Some of it isn’t particularly interesting, so I’ve hidden it behind a code widget. You can unfold it just by clicking.\nThe idea of this page is also that I’ll try and keep the analysis current as more counts come in, while the other pages might slowly grow stale.\n\nImports and initialization\nWe’ll start with some imports, after which we can connect to the database of counts\n\n\nCode for importing packages and connecting to the database\nimport re\nimport sqlite3\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport seaborn as sns\nfrom rcounting import analysis, counters, parsing, side_threads\nfrom rcounting import thread_navigation as tn\nfrom rcounting.reddit_interface import reddit\n\npio.templates.default = \"seaborn\"\nsns.set_theme()\nfrom IPython.display import Markdown\n\ndata_directory = Path(\"../data\")\n\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\n\n\nThen we load some data, both the counts and the gets. We convert the timestamp to a date column, and add a “replying to” column, since some of what we’ll be doing later needs it.\n\n\nCode for loading counts from the counting database\ncounts = pd.read_sql(\n    \"select comments.username, comments.submission_id, comments.timestamp \"\n    \"from comments join submissions \"\n    \"on comments.submission_id = submissions.submission_id \"\n    \"where comments.position > 0 \"\n    \"order by submissions.timestamp, comments.position\",\n    db,\n)\ncounts[\"date\"] = pd.to_datetime(counts[\"timestamp\"], unit=\"s\")\ncounts[\"username\"] = counts[\"username\"].apply(counters.apply_alias)\ncounts.drop(\"timestamp\", inplace=True, axis=1)\ncounts[\"replying_to\"] = counts[\"username\"].shift(1)\nprint(f\"There are {len(counts)} comments logged on main\")\ngets = counts.groupby(\"submission_id\").last().sort_values(\"date\").reset_index()\ngets[\"basecount\"] = (gets.index + 15) * 1000\ngets.loc[[0, 1], [\"basecount\"]] = [0, 16691]\n\n\nThere are 5199937 comments logged on main\n\n\n\n\nCounting progress over time\nA first bit of analysis is to visualize the progress of r/counting over time. That isn’t particularly difficult to do\n\n\nCode to plot the cumulativa counts over time\ndata = gets.set_index(\"date\")\nfig = px.line(\n    data_frame=data.resample(\"30d\")[[\"basecount\"]].mean().reset_index(),\n    x=\"date\",\n    y=\"basecount\",\n    labels={\"basecount\": \"Count\", \"date\": \"Date\"},\n)\nfig.show()\n\n\n\n\n                                                \nFigure 1: Cumulative counts made on /r/counting as a function of time\n\n\n\nOn Figure 1 you can see that the counting rate varies quite a bit over time, with signifcant peaks and lulls in activity. Whether or not there are active runners really changes how fast the count is progressing!\nWe can split the total counts into different groups according to who made them, and plot the total number of counts made over time by the top participants on rcounting.\n\n\nCode to plot the hall of counters\nfrequency = \"24h\"\ntotals = counts.groupby(\"username\").size().sort_values(ascending=False)\ntop_counters = [x for x in totals.index if not counters.is_banned_counter(x)][:25]\nseries = counts.loc[counts[\"username\"].isin(top_counters), [\"username\", \"date\"]]\ntotal = (\n    pd.get_dummies(series.set_index(\"date\")[\"username\"])\n    .resample(frequency)\n    .sum()\n    .cumsum()\n    .melt(ignore_index=False)\n    .reset_index()\n)\n\norder = list(\n    total.groupby(\"variable\")[\"value\"].last().sort_values(ascending=False).index\n)\n\nfig = px.line(\n    data_frame=total,\n    x=\"date\",\n    y=\"value\",\n    line_group=\"variable\",\n    color=\"variable\",\n    category_orders={\"variable\": order},\n    labels={\"date\": \"Date\", \"variable\": \"Counter\", \"value\": \"Total Counts\"},\n)\nfig.show()\n\n\n\n                                                \n\n\n\n\nTotal counts vs k parts\nWe can try plotting thread participation vs total counts. The expectation is that generally, people who’ve made more total counts will also have counted in more threads. However, some users might have periods where they make a count every now and then but never do any runs, leading to comparatively more k_parts. On the other hand, some counters might only do runs, giving a counts/thread of up to 500.\nWe start extract the number of counts and the threads participated in, using the groupby functionality of pandas, and tthen we’ll make a scatter plot of how the two quantities are related. Since the most prolific counters have orders of magnitude more counts and k parts than the least prolific counters, we’ll show both axes on a log scale.\n\n\nCode to plot total counts vs k parts\ngroups = counts.groupby(\"username\")[\"submission_id\"]\nk_parts = groups.nunique()\nhoc = groups.count()\ncombined = pd.concat([k_parts, hoc], axis=1)\ncombined.columns = [\"k_parts\", \"total_counts\"]\ncombined = combined.query(\"k_parts >= 10\")\nfig = px.scatter(\n    data_frame=combined,\n    x=\"k_parts\",\n    y=\"total_counts\",\n    opacity=0.7,\n    log_x=True,\n    log_y=True,\n    trendline=\"ols\",\n    trendline_options=dict(log_x=True, log_y=True),\n    labels={\"k_parts\": \"Threads participated in\", \"total_counts\": \"Total Counts\"},\n)\n\nfig.show()\n\n\n\n\n                                                \nFigure 2: The relationship between the total number of counts for each user, and then number of threads they’ve participated in\n\n\n\nYou can see what that looks like on Figure 2. The line is a linear fit on the log-log plot, and it has a slope of 1.3. In this model, that means that if you double the total number of threads participated in by a user, you would expect to multiply their total counts by 2.5.\n\n\nNumber of partners and effective number of partners\nAs with the number of counts vs threads participated in, we can expect that different counters might have qualitatively different behaviour when it comes to how many counting partners they have, and how often they’ve counted with each one. Some counters might count a little bit with everybody, while others might run with only a few partners, and drop a count with others every now and then.\nTo quantify how uneven the counting distribution is we can look at the effective number of partners of each counter, and compare with the actual number of partners.\n\n\nCode to find the effective and actual number of counting partners\nsorted_counters = counts.groupby(\"username\").size().sort_values(ascending=False)\ntop_counters = [\n    x for x in sorted_counters.index[:35] if not counters.is_banned_counter(x)\n][:30]\ntop = sorted_counters.filter(items=top_counters)\ndf = (\n    counts.loc[counts[\"username\"].isin(top_counters)]\n    .groupby([\"username\", \"replying_to\"])\n    .size()\n)\neffective_partners = (\n    df.groupby(level=0).apply(analysis.effective_number_of_counters).to_frame()\n)\npartners = df.groupby(level=0).count()\ncombined = pd.concat([top, effective_partners, partners], axis=1)\ncombined[\"HOC rank\"] = range(1, len(combined) + 1)\ncombined.columns = [\"counts\", \"c_eff\", \"c\", \"rank\"]\ncombined = combined[[\"rank\", \"c\", \"c_eff\"]]\ncombined.c_eff = combined.c_eff.round().astype(int)\ncombined.columns = [\"HOC rank\", \"N\", \"N_(effective)\"]\ncombined.index.name = \"Username\"\ncombined.head(25)\n\n\n\n\n\n\n  \n    \n      \n      HOC rank\n      N\n      N_(effective)\n    \n    \n      Username\n      \n      \n      \n    \n  \n  \n    \n      Countletics\n      1\n      551\n      8\n    \n    \n      thephilsblogbar2\n      2\n      1503\n      13\n    \n    \n      Antichess\n      3\n      821\n      7\n    \n    \n      davidjl123\n      4\n      1590\n      23\n    \n    \n      GarlicoinAccount\n      5\n      389\n      4\n    \n    \n      Smartstocks\n      6\n      1058\n      29\n    \n    \n      nonsensy\n      7\n      261\n      3\n    \n    \n      TheNitromeFan\n      8\n      1884\n      33\n    \n    \n      atomicimploder\n      9\n      2468\n      30\n    \n    \n      qwertylool\n      10\n      371\n      7\n    \n    \n      ClockButTakeOutTheL\n      11\n      194\n      3\n    \n    \n      Ezekiel134\n      12\n      903\n      13\n    \n    \n      Trial-Name\n      13\n      160\n      4\n    \n    \n      rideride\n      14\n      689\n      11\n    \n    \n      RandomRedditorWithNo\n      15\n      749\n      22\n    \n    \n      Urbul\n      16\n      1087\n      30\n    \n    \n      Mooraell\n      17\n      1208\n      24\n    \n    \n      kdiuro13\n      18\n      848\n      24\n    \n    \n      qualw\n      19\n      267\n      10\n    \n    \n      Removedpixel\n      20\n      1198\n      25\n    \n    \n      TehVulpez\n      21\n      708\n      20\n    \n    \n      Adinida\n      22\n      299\n      9\n    \n    \n      rschaosid\n      23\n      335\n      18\n    \n    \n      LeMinerWithCheese\n      24\n      89\n      7\n    \n    \n      timo78888\n      25\n      320\n      15\n    \n  \n\n\n\n\nWe can also get the replying-to and replied-by stats for a single user\n\n\nCode to calculate reply stats for a single user\ncounter = \"thephilsblogbar2\"\nnick = \"phil\"\nsubset = counts.loc[counts[\"username\"] == counter].copy()\nreplied_by = counts[\"username\"].shift(-1).loc[subset.index]\nsubset[\"replied_by\"] = replied_by\nresult = pd.concat(\n    [\n        subset.groupby(\"replied_by\").count().iloc[:, 0].sort_values(ascending=False),\n        subset.groupby(\"replying_to\").count().iloc[:, 0].sort_values(ascending=False),\n    ],\n    axis=1,\n)\nheaders = [\"Counting partner\", f\"No. of replies by {nick}\", f\"No. of replies to {nick}\"]\nMarkdown(result.head(10).to_markdown(headers=headers))\n\n\n\n\nTable 1: The most popular counting partners of a single user\n\n\n\n\n\n\n\nCounting partner\nNo. of replies by phil\nNo. of replies to phil\n\n\n\n\nGarlicoinAccount\n98821\n98844\n\n\nClockButTakeOutTheL\n51147\n51139\n\n\nCountletics\n35398\n35480\n\n\nAntichess\n17778\n17942\n\n\nTheNitromeFan\n16120\n16222\n\n\natomicimploder\n12176\n12286\n\n\nCutOnBumInBandHere9\n11646\n11744\n\n\nTrial-Name\n11589\n11622\n\n\namazingpikachu_38\n11492\n11484\n\n\nnonsensy\n10543\n10523\n\n\n\n\n\n\n\n\nOldest counters\nWe can see who the oldest still-active counters are, where still-active is generously defined as “having made a count within the last six months”.\n\n\nCode to find the oldest still-active counters\ncutoff_date = pd.to_datetime(\"today\") - pd.Timedelta(\"180d\")\nactive_counters = (\n    counts.loc[counts[\"date\"] > cutoff_date].groupby(\"username\").groups.keys()\n)\noldest_counters = (\n    counts.loc[counts[\"username\"].isin(active_counters)]\n    .groupby(\"username\")[\"date\"]\n    .agg([min, max])\n)\noldest_counters = oldest_counters.sort_values(\"min\").head(25)\nheaders = [\"**username**\", \"**First Count**\", \"**Latest Count**\"]\nMarkdown(oldest_counters.apply(lambda x: x.dt.date).to_markdown(headers=headers))\n\n\n\n\nTable 2: The 25 currently-active counters who’ve been counting for the longest time\n\n\nusername\nFirst Count\nLatest Count\n\n\n\n\nZ3F\n2012-06-10\n2023-04-13\n\n\nCanGreenBeret\n2012-06-12\n2023-01-30\n\n\nOverlordLork\n2012-06-14\n2023-02-17\n\n\n949paintball\n2012-06-15\n2022-12-27\n\n\nmrstickman\n2012-07-18\n2023-03-22\n\n\nkdiuro13\n2013-04-07\n2023-05-04\n\n\nzhige\n2013-05-03\n2023-03-20\n\n\nfalsehood\n2013-06-24\n2023-03-22\n\n\nO_Baby_Baby\n2013-12-13\n2023-02-03\n\n\nCutOnBumInBandHere9\n2013-12-13\n2023-05-03\n\n\nonewhitelight\n2013-12-18\n2023-03-22\n\n\nOreoObserver\n2014-01-25\n2023-01-04\n\n\nD-alx\n2014-02-13\n2023-04-06\n\n\natomicimploder\n2014-02-26\n2023-04-27\n\n\ndude_why_would_you\n2014-02-28\n2023-03-03\n\n\nrideride\n2014-04-07\n2023-04-26\n\n\nJuqu\n2014-04-12\n2023-02-01\n\n\nUebeltank\n2014-06-05\n2023-04-01\n\n\nslockley\n2014-06-23\n2023-01-20\n\n\nBlimp_Blimp\n2014-06-29\n2022-12-13\n\n\nartbn\n2014-07-06\n2023-04-23\n\n\ndavidjl123\n2014-07-16\n2023-05-03\n\n\nMooraell\n2015-01-04\n2023-05-03\n\n\nManiac_34\n2015-01-12\n2023-05-03\n\n\nTheNitromeFan\n2015-01-20\n2023-04-28\n\n\n\n\n\n\n\n\nGets and streaks\nSimilarly to the oldest counters, we can see what the longest difference between a counter’s first and last get is, and that’s shown on Table 3. Some counters have been active and getting gets for quite a while!\n\n\nCode to find the counters with the longest get spans\nMarkdown(\n    gets.groupby(\"username\")\n    .agg(lambda x: x.index[-1] - x.index[0])\n    .iloc[:, 0]\n    .sort_values(ascending=False)\n    .head(10)\n    .to_markdown(headers=[\"**Username**\", \"**Get span**\"])\n)\n\n\n\n\nTable 3: The longest differences between the first and last get of r/counting users (1000s of counts)\n\n\nUsername\nGet span\n\n\n\n\natomicimploder\n4833\n\n\nManiac_34\n4620\n\n\norigamimissile\n4577\n\n\nMooraell\n4563\n\n\nmusicbuilder\n4515\n\n\nPookah\n4500\n\n\nFartyMcNarty\n4373\n\n\nTheNitromeFan\n4305\n\n\nSharpeye468\n4177\n\n\ndavidjl123\n4111\n\n\n\n\n\n\nWe can also calculate what the longest get streaks are.\n\n\nCode to find the longest get streaks\ny = gets[\"username\"]\ngroups = gets.groupby((y != y.shift()).cumsum())\ncolumns = [\"username\", \"submission_id\", \"basecount\"]\nlength = 10\n\nindices = (-groups.size()).sort_values(kind=\"mergesort\").index\nold = groups.first().loc[indices, columns]\nnew = groups.last().loc[indices, columns]\ncombined = old.join(new, rsuffix=\"_new\")\ncombined = (\n    combined.loc[~combined[\"username\"].apply(counters.is_banned_counter)]\n    .head(length)\n    .reset_index(drop=True)\n)\n\n\ndef old_link(row):\n    return (\n        f\"[{int(row.basecount / 1000) + 1}K]\"\n        + f\"(https://reddit.com/comments/{row.submission_id}/)\"\n    )\n\n\ndef new_link(row):\n    return (\n        f\"[{int(row.basecount_new / 1000) + 1}K]\"\n        + f\"(https://reddit.com/comments/{row.submission_id_new}/)\"\n    )\n\n\nheaders = [\"Rank\", \"username\", \"First Get\", \"Last Get\", \"Streak Length\"]\nheaders = [f\"**{x}**\" for x in headers]\ncolumns = [\"username\", \"old_link\", \"new_link\", \"streak\"]\ncombined[\"old_link\"] = combined.apply(old_link, axis=1)\ncombined[\"new_link\"] = combined.apply(new_link, axis=1)\ncombined[\"streak\"] = 1 + (combined[\"basecount_new\"] - combined[\"basecount\"]) // 1000\ncombined.index += 1\ncombined.index.name = \"Rank\"\nMarkdown(combined[columns].to_markdown(headers=headers))\n\n\n\n\nTable 4: The longest streak\n\n\nRank\nusername\nFirst Get\nLast Get\nStreak Length\n\n\n\n\n1\nAntichess\n5093K\n5115K\n23\n\n\n2\nCountletics\n2896K\n2914K\n19\n\n\n3\nLeMinerWithCheese\n4570K\n4584K\n15\n\n\n4\nCountletics\n5187K\n5199K\n13\n\n\n5\ndavidjl123\n3038K\n3047K\n10\n\n\n6\nCountletics\n3091K\n3100K\n10\n\n\n7\nCountletics\n3190K\n3199K\n10\n\n\n8\nCountletics\n4384K\n4393K\n10\n\n\n9\nCountletics\n5143K\n5151K\n9\n\n\n10\nqualw\n2042K\n2049K\n8\n\n\n\n\n\n\nThe core of the extraction is the line that says groups = gets.groupby((y != y.shift()).cumsum()). Let’s unpack it:\n\ny != y.shift() assigns a value of True to all threads with a username that’s different from their predecessor\n.cumsum() sums up all these True values. The net result is that each get streak is given its own unique number\n.groupby() extracts these groups for later use\n\nThe groups are then sorted according to size, and the largest ones are shown in Table 4"
  },
  {
    "objectID": "distribution.html",
    "href": "distribution.html",
    "title": "The Distribution of Counts or: How I Learned to Stop Worrying and Love Pandas Indexing",
    "section": "",
    "text": "Introduction\nThis round of analysis is a deep dive into the distribution of counts and counters, across the thousands of threads that have been posted on r/counting over the years. I’ll be trying to answer questions like “Which number from 1-1000” has been counted by the most people, and “Which people have a significant preference for counting odd or even numbers”.\nWe’ll start off with some code to import the relevant packages and load the data.\n\n\nCode for importing packages and loading data\nfrom pathlib import Path\nimport pandas as pd\nimport sqlite3\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom rcounting import counters, analysis, graph_tools\nimport seaborn as sns\nsns.set_theme()\nfrom IPython.display import Markdown\ndata_directory = Path(\"../data\")\nimport itertools\n\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\ncounts = pd.read_sql(\"select username, submission_id, position from comments where position > 0 order by timestamp\", db)\ncounts[\"username\"] = counts[\"username\"].apply(counters.apply_alias)\ncounts[\"position\"] = (counts[\"position\"] - 1) % 1000 + 1\n\n\n\n\n100% Completion\nA natural place to start is to look at how many counters have counted every number from 1 to 1000. For this (and most of the other stats in this page) I’ll be restricting my attention to people who have made at least 1000 counts.\n\n\nCode\ntotals = counts.groupby(\"username\").size()\nusernames = [x for x in totals[totals >= 1000].index if not counters.is_banned_counter(x)]\nsubset = counts.query(\"username in @usernames\").copy()\n\nmissing_values = 1000 - subset.groupby(\"username\")[\"position\"].nunique()\nsum(missing_values == 0)\n\n\n80\n\n\nWe can drill a little deeper and ask how quickly each counter reached the full set. In theory it’s possible to do that after only 1000 counts, but in practice it’s extremely unlikely. Indeed, Table 1 shows that even the fastest counters needed multiple times that.\n\n\nCode\ncompletists = [username for username in usernames if missing_values[username] == 0]\ncomplete = subset.query(\"username in @completists\").copy()\ncomplete[\"index\"] = complete.index\nlast_counts = complete.groupby(\"username\").apply(lambda x: (x.groupby(\"position\").head(1)).tail(1)[\"index\"])\ntotals = counts.groupby(\"username\").cumcount()\ndf = (last_counts\n      .reset_index(level=1)[\"index\"]\n      .apply(lambda x: totals.loc[x])\n      .sort_values())\nheaders = [\"**Username**\", \"**Number of Counts**\"]\nMarkdown(pd.concat([df.tail().sort_values(ascending=False), df.head()]).to_markdown(headers=headers))\n\n\n\n\nTable 1: The fastest and slowest counters to count every number from 1 to 1000, according to how many counts they had when they reached the full sets\n\n\nUsername\nNumber of Counts\n\n\n\n\nbluesolid\n12272\n\n\nKingCaspianX\n11706\n\n\nRemovedpixel\n9979\n\n\nSmartstocks\n9904\n\n\n_zachhall\n8993\n\n\nnoduorg\n2865\n\n\nCountletics\n2963\n\n\nMorallyGray\n3606\n\n\nChristmas_Missionary\n3735\n\n\nSharpeye468\n3794\n\n\n\n\n\n\nSimilarly, we can take a look at which counters are only missing a few values before they have the full set:\n\n\nCode\nmissing = [username for username in usernames if 0 < missing_values[username] <= 5]\ndf = (subset\n      .query(\"username in @missing\")\n      .groupby(\"username\")\n      .size()\n      .sort_values(ascending=False)\n      .to_frame(name=\"n_counts\"))\ndf[\"missing_value\"] = (subset\n                       .query(\"username in @missing\")\n                       .groupby(\"username\")[\"position\"]\n                       .apply(lambda x: (set(range(1, 1001)) - set(x.unique()))))\nheaders = [\"**Username**\", \"**Total Counts**\", \"**Missing Values**\"]\nMarkdown(df.head(10).to_markdown(headers=headers))\n\n\n\nThe 10 counters with the most counts who are still missing up to five values in order to have counted the full set of numbers from 1 to 1000 \n\n\nUsername\nTotal Counts\nMissing Values\n\n\n\n\nwhit4you\n11663\n{998}\n\n\nidunnowhy9000\n10824\n{952, 999}\n\n\n949paintball\n10611\n{995, 991}\n\n\nMetArtScroll\n9923\n{56, 54}\n\n\nKrazeli\n8303\n{280}\n\n\nbasscrow\n7416\n{999}\n\n\nNekyiia\n6706\n{11, 5}\n\n\ngordonpt8\n5953\n{128}\n\n\nteddymaniacc\n5788\n{379, 9, 11}\n\n\ncob331\n5143\n{1000}\n\n\n\n\n\n\n\nEfficient getters\nThe counts on r/counting are organised in threads of 1000 counts, and getting the last count on a thread is a bit of a prize. Some counters are very motivated by gets, and some counters are less motivated by them, but everyone is aware of them.\nOn average, everyone should have made 1000 counts for each get they have, but some counters have ratios that are significantly different from that. We can look at which counters have made the fewest total counts but still managed to obtain a get\n\n\nCode\ngetters = counts.groupby(\"submission_id\").last()\ngets = getters.reset_index()['username'].value_counts()\ngets.name = \"n_gets\"\ntotals = pd.concat([counts['username'].value_counts().loc[gets.index], gets], axis=1)\ntotals.columns = [\"counts\", \"gets\"]\nheaders = [\"**Username**\", \"**Counts**\", \"**Gets**\"]\nMarkdown(totals.sort_values(by='counts').head().to_markdown(headers=headers))\n\n\n\nThe counters with the fewest total number of counts who still have at least one get. \n\n\nUsername\nCounts\nGets\n\n\n\n\nthephilsnipebar\n1\n1\n\n\nMeNowDealWithIt\n1\n1\n\n\nHotshot2k4\n3\n1\n\n\nRalph_Schaosid\n5\n1\n\n\nItzTaken\n5\n1\n\n\n\n\n\nLooking at this list, thephilsnipebar and Ralph_Schaosid are immediately obvious as counting alts. MeNowDealWithIt has made significantly more than one count, but deleted their account before they were picked up by the script, so I have no idea how many. Hotshot2k4 seems legit. They made three counts, the first of which was the 54k get. More recently, ItzTaken got a free get that VitaminB16 left in the 4195k thread. Before that they had made two counts in the 2M era.\n\n\nCode\nlt_100 = (totals[\"counts\"] < 100).sum()\nlt_1000 = (totals[\"counts\"] < 1000).sum()\ns = (f\"All in all we have ~{round(lt_100, -1)} counters with a get and less than 100 total counts \"\n     f\"and ~{round(lt_1000, -1)} with less than 1000 counts. \"\n     \"But again, there's a significant number of counts where I don't know the author, \"\n     \"and a significant number of usernames that are unknown aliases.\")\nMarkdown(s)\n\n\nAll in all we have ~30 counters with a get and less than 100 total counts and ~180 with less than 1000 counts. But again, there’s a significant number of counts where I don’t know the author, and a significant number of usernames that are unknown aliases.\n\n\nMaybe it’s better instead to look only at counters who have made at least 1000 counts. The relevant comparison would then be the ratio of counts to gets. That’s shown on Table 2, and veterans of r/counting will recognise some of the first five names as counters who like to try and snipe the get.\n\n\nCode\ntotals = totals.loc[totals[\"counts\"] >= 1000].copy()\ntotals[\"ratio\"] = totals[\"counts\"] / totals[\"gets\"]\ntotals = totals.sort_values(by=\"ratio\")\nheaders = [\"**Username**\", \"**Counts**\", \"**Gets**\", \"**Counts / Gets**\"]\nMarkdown(pd.concat([totals.head(), totals.tail()]).to_markdown(headers=headers))\n\n\n\n\nTable 2: The counters with at least 1000 counts who have the lowest and highest ratio of counts to gets.\n\n\nUsername\nCounts\nGets\nCounts / Gets\n\n\n\n\nboxofkangaroos\n3704\n38\n97.4737\n\n\nManiac_34\n10688\n70\n152.686\n\n\nDamnified\n1566\n7\n223.714\n\n\nSharpeye468\n6816\n30\n227.2\n\n\nUnknow3n\n1688\n7\n241.143\n\n\na-username-for-me\n10420\n2\n5210\n\n\nidunnowhy9000\n10824\n2\n5412\n\n\nzvrizdinus\n5746\n1\n5746\n\n\nwhit4you\n11663\n2\n5831.5\n\n\n949paintball\n10611\n1\n10611\n\n\n\n\n\n\n\n\nThe Overall Counting Distribution\nI promised to write about a distribution of counts, and so far I’ve mainly written about what numbers individual counters have or have not counted. And there hasn’t been a single graph yet! But I promise that’s about to change. A fun thing to look at first is how many people have counted each number from 1 to 1000. It wouldn’t be far-fetched to assume that each number had been counted by roughly the same amount of people, but that’s not at all what happens. Figure 1 has the details.\n\n\nCode\naggregated = counts.groupby(\"position\")[\"username\"].nunique()\nax = aggregated.iloc[0:1000].plot(ylabel=\"Number of different counters\", xlabel=\"Thread position\")\n_ = ax.set_xlim(-5, 1000)\n\n\n\n\n\nFigure 1: The amount of people who have counted each number. You can see a very sharp rise from the start of each thread to ~50, followed by a steady decline towards the get. The most popular number has been counted by more than twice as many people as the least popular.\n\n\n\n\nWe can also look at the counting distributions for individual counters. Again, the default assumption would be that everybody has counted each number roughly the same number of times. Not too surprisingly, we see that this assumption holds better for some counters than it does for others. Figure 2 shows the counting distributions for the most and least regular counter, and you can really see the difference between the two.\nThe graph has been split into odds and evens, because there’s generally a consistent difference between those two series. Intuitively, that makes sense, since most counts are made in runs where a given user makes every second counts. It’s therefore not too strange that the behaviour at a value \\(n\\) is more similar to that at \\(n - 2\\) than at \\(n - 1\\).\n\n\nCode\ngrouped = complete[['username', 'position']].value_counts()\ncov = grouped.groupby(level=0).agg(lambda x: np.std(x) / np.mean(x)).sort_values()\nusers = [cov.index[0], cov.index[-1]]\ndata = grouped.loc[users].sort_index().to_frame().reset_index()\ndata.columns = ['Counter', 'Thread Position', 'count']\ndata.loc[data[\"Counter\"] == users[0], 'count'] /= data.loc[data[\"Counter\"] == users[0], 'count'].mean()\ndata.loc[data[\"Counter\"] == users[1], 'count'] /= data.loc[data[\"Counter\"] == users[1], 'count'].mean()\ndata[\"Parity\"] = [[\"even\", \"odd\"][val] for val in data.index % 2]\nax = sns.lineplot(data=data, y='count', x=\"Thread Position\", hue=\"Parity\", style=\"Counter\")\n_ = ax.set_ylabel(\"Relative frequency\")\n\n\n\n\n\nFigure 2: The normalized number of counts made at each value for the most and least regular counters. If every number had been counted exactly the same amount of times, there would be a flat line at y=1\n\n\n\n\nWe can quantify the difference for each counter through the Coefficient of Variation, which expresses how far their counting distribution is from uniform. Here’s a table of the five most and five least regular counters:\n\n\nCode\nMarkdown(pd.concat([100*cov.head(), 100*cov.tail()])\n         .to_markdown(headers=[\"**Username**\", \"**Coefficient of Variation [%]**\"]))\n\n\n\n\n\nUsername\nCoefficient of Variation [%]\n\n\n\n\nCountletics\n9.41016\n\n\nEzekiel134\n9.58323\n\n\nRandomRedditorWithNo\n9.90369\n\n\nAntichess\n9.90815\n\n\natomicimploder\n10.3837\n\n\na-username-for-me\n46.2564\n\n\nThreeDomeHome\n48.1319\n\n\nbluesolid\n51.7566\n\n\n_zachhall\n51.9715\n\n\nmistyskye14\n55.9725\n\n\n\n\n\nOf course, we saw from Figure 1 that there’s a significant variation in how many people have counted each number, so perhaps the uniform distribution is a bad model for how often we should expect each counter to have counted a particular number. Indeed, since twice as many people have counted the number 50 as have counted 1000, then on average people who have counted 1000 have done so twice as often as people who have counted 50. That leads to a model distribution that goes as \\(f(n) \\propto \\frac{1}{\\textrm{number of people who have counted n}}\\). We can again look through all the counters and see who has a counting distribution closest to this ideal:\n\n\nCode\ndistribution = 1 / aggregated.iloc[0:1000]\ndistribution = distribution / distribution.mean()\nl2 = (complete[[\"username\", \"position\"]]\n      .value_counts()\n      .groupby(level=0)\n      .agg(lambda x: ((x / x.mean() - distribution)**2).sum()))\nusername = l2.sort_values().index[0]\ndata = (complete[['username', 'position']]\n        .value_counts()\n        .loc[username]\n        .sort_index()\n        .to_frame()\n        .reset_index())\ndata.columns = ['Thread Position', 'count']\ndata['count'] /= data['count'].mean()\ndata[\"Parity\"] = [[\"even\", \"odd\"][val] for val in data.index % 2]\nax = sns.lineplot(data=data, y='count', x=\"Thread Position\", hue=\"Parity\", lw=2, linestyle=\"--\")\nax.plot(distribution, zorder=-1, color=\"0.7\")\nax.set_ylabel(\"Relative Frequency\")\n_ = ax.set_title(username)\n\n\n\n\n\nFigure 3: The distribution of counts for the counter who most closely matches the model distribution.\n\n\n\n\nIt’s impressive just how closely david’s counting frequency matches the toy model I suggested above!\n\n\nOdds and evens\nIn the previous section we saw that for some counters, there’s a significant difference between how they’ve counted the odd numbers, and how they’ve counted the even numbers. The difference is not unexpected, since the nature of counting means that for any given run you’ll be stuck on either the odd numbers or the even numbers.\nIt is striking though just how large the difference can be for some counters, so here’s a table of the most odd counters, the most even counters and the most balanced counters:\n\n\nCode\ncounts[\"is_even\"] = (counts[\"position\"] % 2 == 0)\noffsets = [\"1gm10t\", \"7hn2tm\", \"b471wg\", \"bz6r0g\", \"d6pgni\", \"ebnh39\", \"grggc0\", \"oj50hj\", \"ob4a2h\", \"t81gug\"]\nfor offset in offsets:\n    counts.loc[counts[\"submission_id\"] == offset, 'is_even'] = 1 - counts.loc[counts[\"submission_id\"] == offset, 'is_even']\ncounts['is_odd'] = 1 - counts['is_even']\nsubset = counts.query(\"username in @usernames\")\ntable = subset[['username', 'is_even', 'is_odd']].groupby('username').sum()\ntable.columns=[\"n_even\", \"n_odd\"]\ntable['difference'] = table['n_even'] - table['n_odd']\ntable['relative_difference'] = (table['n_even'] - table['n_odd']) / (table['n_even'] + table['n_odd']) * 100\ntable['absolute_difference'] = abs(table['relative_difference'])\nheaders=[\"**Username**\", \"**n_(even)**\", \"**n_(odd)**\", \"**Difference**\", \"**Relative Difference [%]**\"]\ncolumns = ['n_even', 'n_odd', 'difference', 'relative_difference']\nMarkdown(pd.concat([table.sort_values(by='difference').head(),\n                    table.sort_values(by='difference', ascending=False).head(),\n                    table.sort_values(by='absolute_difference').head()])[columns]\n         .to_markdown(headers=headers))\n\n\n\n\nTable 3: Three sets of counters, organised by parity: Those with most odd counts, those with most even counts, and those who are closest to being perfectly balanced.\n\n\n\n\n\n\n\n\n\nUsername\nn_(even)\nn_(odd)\nDifference\nRelative Difference [%]\n\n\n\n\nthephilsblogbar2\n196661\n246743\n-50082\n-11.2949\n\n\nSmartstocks\n85516\n116356\n-30840\n-15.277\n\n\nTrial-Name\n34744\n47309\n-12565\n-15.3133\n\n\nrschaosid\n19358\n27815\n-8457\n-17.9276\n\n\ncolby6666\n12448\n20211\n-7763\n-23.7699\n\n\nGarlicoinAccount\n113830\n94681\n19149\n9.18369\n\n\nCountletics\n207496\n188802\n18694\n4.71716\n\n\nLeMinerWithCheese\n27467\n13503\n13964\n34.0835\n\n\nqwertylool\n56823\n47547\n9276\n8.88761\n\n\nClockButTakeOutTheL\n39636\n33606\n6030\n8.23298\n\n\nYnax\n7613\n7615\n-2\n-0.0131337\n\n\ncob331\n2572\n2571\n1\n0.0194439\n\n\nsupersammy00\n7886\n7900\n-14\n-0.0886862\n\n\nnonsensy\n85603\n85441\n162\n0.0947125\n\n\nanimus777\n819\n817\n2\n0.122249\n\n\n\n\n\n\nThat’s all for now!"
  },
  {
    "objectID": "dst.html",
    "href": "dst.html",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "",
    "text": "In a previous post I looked at the daily rhythm of r/counting, at what time of day the subreddit is most active, and how that has changed throughout the years. In this post I will try and answer the question that’s on nobody’s lips: Is it possible to see the effect of daylight saving time on the r/counting schedule?\nReddit stores the UTC timestamp for every count, so it’s possible to compare the counting activity just before the start of DST with the counting activity just afterwards, and see whether there’s a difference. If counts always follow the same pattern in local time, and all counters observe DST at the same time, then that should show up as a rigid shift in the data.\nThat’s the theory at least. As a spoiler, I’m doing something a bit different in this post, so it’s going to seem a lot more open-ended and exploratory than some of the other ones. When I started coding and writing I didn’t entirely know where I was going to end up, so you can think of this as going on a journey with me. Apart from answering the question, my goal with this post is to show you my general approach for dealing with these kinds of questions, and to highlight some of my thoughts along the way.\nI’ll start off with some code to import the relevant packages and load the data."
  },
  {
    "objectID": "dst.html#adding-more-weeks",
    "href": "dst.html#adding-more-weeks",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "Adding more weeks",
    "text": "Adding more weeks\nLet’s start by looking at what happens before DST is active. For the preceding analysis to be valid, the distribution of counts throughout the day would need to be basically the same in the two weeks before the start of DST.\n\n\nCode\nfor week in [\"Without DST\", \"Control without DST\"]:\n    df = spring_kdes.query(\"week_name == @week\")\n    plt.fill_between(df[\"time\"], df[\"rate\"], alpha=0.8, label=week)\nax = plt.gca()\nplots.make_time_axis(ax)\nax.set_ylabel(\"Counting rate (arbitrary units)\")\nax.set_xlabel(\"Time of Day (UTC - 5)\")\nax.legend()\nplt.show()\n\n\n\n\n\nHm. Those two curves might be slightly more aligned than the two with and without DST, but it’s not super clear. I can check the optimal shift\n\n\nCode\n_, optimal_shift = calculate_shifted_overlap(spring_kdes, \"Without DST\", \"Control without DST\")\nprint(f\"The optimal shift is {int(optimal_shift * BIN_TO_MINUTE)} minutes.\")\n\n\nThe optimal shift is 98 minutes.\n\n\nThat’s an even bigger shift than the one that happened when DST was introduced! I can plot the four curves for the two weeks before and after DST together and see if there’s any obvious pattern.\n\n\nCode\nspring_kdes[\"shifted_rate\"] = spring_kdes[\"rate\"] + (spring_kdes[\"week\"] + 2) * dy\nax = sns.lineplot(spring_kdes, x=\"time\", y=\"shifted_rate\", hue=\"week_name\")\nax.legend_.set_title(\"Week\")\nplots.make_time_axis(ax)\nax.legend(loc=\"upper center\", ncol=2)\nax.set_ylabel(\"Counting rate (arbitrary units)\")\nax.set_xlabel(\"Time of Day (UTC - 5)\")\nax.set_ylim(0, 0.34)\nplt.show()\n\n\n\n\n\nIf you didn’t have the legend, would you be able to tell which two of these curves were with DST and which were without? It seems that the variation from week to week is so big that any DST signal that might be present in the data is just swamped by all the noise."
  },
  {
    "objectID": "dst.html#including-the-end-of-dst",
    "href": "dst.html#including-the-end-of-dst",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "Including the end of DST",
    "text": "Including the end of DST\nI can try and see if including the data for the end of DST makes any difference\n\n\nCode\nautumn_all = wrangle(pd.concat([pd.read_sql(query(x), db) for x in dst_end.values()]), dst_end)\nautumn = autumn_all[mask(autumn_all)].copy()\nautumn[\"week\"] = -1 - autumn_all[\"week\"]\nkdes = generate_kdes(pd.concat([spring, autumn]))\nkdes[\"week_name\"] = kdes[\"week\"].map(week_map)\nkdes[\"shifted_rate\"] = kdes[\"rate\"] + (kdes[\"week\"] + 2) * dy\nax = sns.lineplot(kdes, x=\"time\", y=\"shifted_rate\", hue=\"week_name\")\nax.legend_.set_title(\"Week\")\nplots.make_time_axis(ax)\nax.legend(loc=\"upper center\", ncol=2)\nax.set_ylabel(\"Counting rate (arbitrary units)\")\nax.set_xlabel(\"Time of Day (UTC - 5)\")\nax.set_ylim(0, 0.34)\nax.set_yticks([])\n_, optimal_shift = calculate_shifted_overlap(kdes, \"With DST\", \"Without DST\")\nprint(f\"The optimal shift is {int(optimal_shift * BIN_TO_MINUTE)} minutes.\")\nplt.show()\n\n\nThe optimal shift is 17 minutes.\n\n\n\n\n\nFigure 1: The aggregated activity on r/counting in the two weeks on either side of the start/end of DST.\n\n\n\n\nAs before – would you be able to tell which of these graphs were with DST and which were without if you didn’t have the legend?"
  },
  {
    "objectID": "dst.html#summing-up",
    "href": "dst.html#summing-up",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "Summing up",
    "text": "Summing up\nThe validation of the model has revealed that the activity on r/counting varies enough on a week to week basis that my initial assumptions are incorrect, and I can’t just treat the activity as a constant background with a DST signal on top. If I want to see the effect of DST, I’m going to have to come up with something more clever."
  },
  {
    "objectID": "dst.html#disaggregating-the-years",
    "href": "dst.html#disaggregating-the-years",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "Disaggregating the years",
    "text": "Disaggregating the years\nWhat I did in the previous section was to aggregate the activity on r/counting across all the years it’s been active. After that, I honed in on specific weeks near the time of year when the clocks change, and asked if there was a rigid shift in the data.\nThis analysis revealed that the activity on r/counting isn’t stable over time. Maybe I’m losing information by aggregating all the years, and the signal would be clearer if I looked at each year separately.\nBefore I can make these comparisons I’m going to need a way of boiling down the information. Figure 1 and friends in the previous section showed that spotting the shift by eye is very difficult, and if the plot is further split into a new line for each year, it’s going to become completely unreadable.\nWhat I need is a way of compressing each (week, year) pair to a single point, so that the plots are still legible even after disaggregating the years.\nI can use the fac t that the DST offset is exactly one hour to accomplish just this: For each week, I can calculate how much the distribution resembles that of the week before, and I can also calculate how much the distribution resembles the 1 hour shifted distribution from the week before.\nFor most of the year, it should be the case that the unshifted distribution is more similar then the shifted distribution. But, for the week where the clocks change, the shifted distribution should be more similar. So, I can calculate the similarity of the lagged and shifted distribution, and subtract the similarity of just the lagged distribution, and I have a DST fingerprint. For most weeks, it should give a negative value, but for the week where the clocks change it should give a positive value.\nLet’s see how it goes!\n\n\nCode\ndef dst_fingerprint(df, period=\"spring\"):\n    \"\"\"Calculate the dst fingerprint for a single year\"\"\"\n    transitions = dst_start if period == \"spring\" else dst_end\n    x = df.resample(\"300s\", on=\"date\").size()\n    rates = x.div(x.groupby(pd.Grouper(freq=\"1d\")).transform(\"sum\")).to_frame(name=\"rate\")\n    rates.index = rates.index - pd.to_datetime(rates.index.year.map(transitions), unit=\"s\")\n    shifted = rates.shift(freq=\"7d\")\n    shift = \"-1h\" if period == \"spring\" else \"1h\"\n    dst_shifted = shifted.shift(freq=shift)\n\n    dfs = []\n\n    for df in [shifted, dst_shifted]:\n        background = pd.Series((maxval - minval) * [np.nan], range(minval, maxval), name=\"delta\")\n        background.index.name = \"date\"\n        f1 = pd.merge(rates, df, left_index=True, right_index=True)\n        if len(f1) != 0:\n            f1[\"delta\"] = (f1[\"rate_x\"] - f1[\"rate_y\"]) ** 2\n            series = f1.groupby(f1.index.days // 7)[\"delta\"].sum()\n            background.loc[series.index] = series\n        dfs.append(background)\n\n    return dfs[1] - dfs[0]\n\n\ndef multiple_dst_fingerprints(df, period=\"spring\"):\n    groups = df.groupby(\"year\").apply(dst_fingerprint, period=period)\n    return groups.reset_index().melt(id_vars=\"year\")\n\n\nweek_norm = spring_all.groupby([\"year\", \"username\"]).size() / spring_all.groupby(\"year\").size()\nyear_norm = spring_all.groupby(\"year\").size() / len(spring_all)\n\ndf = multiple_dst_fingerprints(spring_all)\nax = sns.scatterplot(df, x=\"date\", y=\"value\", hue=\"year\", palette=\"plasma\")\nsns.lineplot(\n    df.dropna().set_index([\"year\", \"date\"])[\"value\"].mul(year_norm).groupby(level=1).sum(),\n    color=\"0.4\",\n    legend=False\n)\n\nax.axhline(0, color=\"0.5\", linestyle=\"--\")\nax.set_xlabel(\"Weeks after start of DST\")\nax.set_ylabel(\"DST fingerprint\")\nplt.show()\n\n\n\n\n\nHm. This isn’t very promising. The DST signal should show up in this plot in the fact that the points at 0 should lie significantly higher than all the others. That’s not the case at all.\nI can do the same thing for when DST ends, just for good measure, to see if the signal shows up there:\n\n\nCode\ndf = multiple_dst_fingerprints(autumn_all, \"autumn\")\nax = sns.scatterplot(df, x=\"date\", y=\"value\", hue=\"year\", palette=\"plasma\")\nsns.lineplot(\n    df.dropna().set_index([\"year\", \"date\"])[\"value\"].mul(year_norm).groupby(level=1).sum(),\n    color=\"0.4\",\n    legend=False\n)\nax.axhline(0, color=\"0.5\", linestyle=\"--\")\nax.set_xlabel(\"Weeks after end of DST\")\nax.set_ylabel(\"DST fingerprint\")\nplt.show()\n\n\n\n\n\nUnfortunately, that didn’t seem to show the signal either. Before giving up completely and abandoning this as a fool’s errand, there’s one or two more things I can try."
  },
  {
    "objectID": "dst.html#disaggregating-the-different-counters",
    "href": "dst.html#disaggregating-the-different-counters",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "Disaggregating the different counters",
    "text": "Disaggregating the different counters\nRegulars of r/counting will know that it’s not the same people who count every week. If proof of this is needed, you can take a look at the top weekly counters list and see that it really isn’t just a repeat from week to week. Perhaps this is one cause of the lack of pattern in the counting times. It’s certainly possible to imagine a world where counters are perfectly regular, but the different schedules of different counters coupled with their different activity from week to week adds up to a huge mess.\nSo I can keep going with the disaggregation, and see if I get a clearer signal when we compare the activity of individual counters from week to week.\n\n\nCode\ndef username_fingerprint(df, period=\"spring\"):\n    fingerprint = (df.groupby([\"year\", \"username\"])\n                   .apply(dst_fingerprint, period=period)\n                   .reset_index()\n                   .melt(id_vars=[\"year\", \"username\"], var_name=\"week\")\n                   .set_index([\"year\", \"username\", \"week\"])[\"value\"])\n    return fingerprint\n\nfingerprints = username_fingerprint(spring_all)\ndf = (\n    fingerprints.dropna()\n    .mul(week_norm)\n    .groupby(level=[0, 2])\n    .sum()\n    .reset_index(name=\"fingerprint\")\n)\nax = sns.scatterplot(df, x=\"week\", y=\"fingerprint\", hue=\"year\", palette=\"plasma\")\nsns.lineplot(\n    df.set_index([\"year\", \"week\"])[\"fingerprint\"].mul(year_norm).groupby(level=1).sum(),\n    ax=ax,\n    color=\"0.4\",\n    legend=False,\n)\nax.axhline(0, color=\"0.5\", linestyle=\"--\")\n\nax.set_xlabel(\"Weeks after start of DST\")\nax.set_ylabel(\"DST fingerprint\")\nplt.show()\n\n\n\n\n\nLooking at this graph almost makes me think I have a sign error in the way I’ve defined the DST fingerprint. I’ve double checked, and I don’t think it’s the case, but this certainly isn’t the peak at 0 I was hoping to see."
  },
  {
    "objectID": "dst.html#looking-only-at-the-most-regular-counters",
    "href": "dst.html#looking-only-at-the-most-regular-counters",
    "title": "Daylight Saving Time: On modelling and robustness",
    "section": "Looking only at the most regular counters",
    "text": "Looking only at the most regular counters\nNone of what I’ve tried so far has seemed to work. There’s one last thing I can try: I can find out which counters were most regular in the period leading up to the start of DST each year, and only include them in the calculations\nTo do that I’ll need to slightly rework the code from Section 1 for calculating the overlap between two different counting distributions. This will let me calculate the overlap for every counter for every year in the weeks around the onset of DST.\nThen it’s just a bit of fidding with indices to find the 5 most regular counters two weeks before the start of DST for every year.\n\n\nCode\ndef similarity_score(df):\n    kdes = generate_kdes(df)\n    groups = kdes.groupby(\"week\")[\"rate\"]\n    norm = groups.transform(np.linalg.norm)\n    kdes[\"rate\"] /= norm\n    overlaps = (\n        (kdes.set_index([\"week\", \"time\"]).groupby(\"time\")[\"rate\"].diff() ** 2)\n        .groupby(level=0)\n        .sum()\n    )\n    return 1 - overlaps / 2\n\nscores = spring_all.groupby([\"year\", \"username\"]).apply(similarity_score)\nscores = scores[scores != 1]\ncounters = (\n    scores\n    .reset_index()\n    .query(\"week== -2\")\n    .sort_values([\"year\", \"rate\"], ascending=False)\n    .groupby(\"year\")\n    .head(5)\n    .set_index([\"year\", \"username\"])\n    .index\n)\nsubset = spring_all.set_index([\"year\", \"username\"]).loc[counters]\n\n\nWith that out of the way, I can plot the average similarity score for each year and week\n\n\nCode\nweek_norm = subset.groupby([\"year\", \"username\"]).size() / subset.groupby(\"year\").size()\nyear_norm = subset.groupby(\"year\").size() / len(subset)\n\nsimilarity = (\n    scores.reset_index(level=2)\n    .loc[counters]\n    .set_index(\"week\", append=True)[\"rate\"]\n    .mul(week_norm)\n    .groupby(level=[0, 2])\n    .sum()\n    .reset_index(name=\"similarity\")\n)\n\nax = sns.scatterplot(similarity, x=\"week\", y=\"similarity\", hue=\"year\", palette=\"plasma\")\nsns.lineplot(\n    (\n        similarity.set_index([\"year\", \"week\"])[\"similarity\"]\n        .mul(year_norm)\n        .groupby(\"week\")\n        .sum()\n    ),\n    color=\"0.4\",\n    ax=ax,\n    legend=False\n)\nax.set_xlabel(\"Weeks after start of DST\")\nax.set_ylabel(\"Week consistency score\")\nplt.show()\n\n\n\n\n\nI’ve decided to work with the consistency score rather than the DST fingerprint3, because I wanted to highlight the effect of the selection I’ve made. You can see on the graph that the score two weeks before the start of DST is signficantly higher than all the other weeks, and in particular the least consistent year for week -2 is much more consistent tht the least consistent year for all the other weeks.3 The two scores behave similarly, with the exception that for the consistency score, we’d expected DST to show up as a dip at zero, rather than a peak.\nThis apparent result is just an artefact of the way I’ve selected the data: I’ve limited my search to people who were very consistent two weeks before the onset of DST, so it’s no surprise that the consistency is high here. The fact that the consistency is lower in the following weeks is due to a regression towards the mean.\nThat statistical artefact aside, it doesn’t seem that this analysis has brought me any closer to finding a clear sign of DST in the counting data. With the consistency score, the sign of DST is a dip at 0, so the fact that the least consistent week is the week DST starts is suggestive. But it’s not what I’d call proof."
  },
  {
    "objectID": "side_threads.html",
    "href": "side_threads.html",
    "title": "All about side threads",
    "section": "",
    "text": "This is a brief post on some preliminary work I’ve done looking at the side threads on rcounting. The post is split into two sections. First, how I’ve worked with the log files to get them into a structured database that’s easier for me to reason about, and after that, how I’veused the logs to plot two different charts: A cumulative total of the number of counts made in each thread, and the most popular side thread over time.\nIt’s also the first time I’m working with the plotly library to make an interactive graph, so please let me know if you have any comments on that!"
  },
  {
    "objectID": "side_threads.html#the-most-prolific-side-thread-counters",
    "href": "side_threads.html#the-most-prolific-side-thread-counters",
    "title": "All about side threads",
    "section": "The most prolific side thread counters",
    "text": "The most prolific side thread counters\nWe can use basically the same approach to find and plot the top side thread counters over time,\n\n\nCode to plot the hall of side threads\ncounts = comments.groupby(\"username\").size().sort_values(ascending=False)\ntop_counters = [x for x in counts.index if not counters.is_banned_counter(x)][:25]\ndf = comments.loc[comments[\"username\"].isin(top_counters), [\"username\", \"timestamp\"]]\ntotal = (\n    pd.get_dummies(df.set_index(pd.to_datetime(df[\"timestamp\"], unit=\"s\"))[\"username\"])\n    .resample(frequency)\n    .sum()\n    .cumsum()\n    .melt(ignore_index=False)\n    .reset_index()\n    .rename(\n        {\"timestamp\": \"Date\", \"variable\": \"Counter\", \"value\": \"Total Counts\"}, axis=1\n    )\n)\n\nfig = px.line(\n    data_frame=total,\n    x=\"Date\",\n    y=\"Total Counts\",\n    line_group=\"Counter\",\n    color=\"Counter\",\n    category_orders={\"Counter\": top_counters},\n)\nfig.show()\n\n\n\n                                                \n\n\nA couple of things stand out about this plot too. The first is how consistent u/TheNitromeFan’s counting rate was between late 2017 and the start of 2022, followed by his semi-retirement since then. Similarly, you can see how atomicimploder bascially left the subreddit for a couple of years before coming back to reclaim his number 2 spot in the total number of side thread counts.\nIt’s also fun to see how many of our counters have wildly varying rates of side thread activity over time, which makes for bumpy lines on this plot."
  },
  {
    "objectID": "side_threads.html#plotting-the-most-popular-side-thread-over-time",
    "href": "side_threads.html#plotting-the-most-popular-side-thread-over-time",
    "title": "All about side threads",
    "section": "Plotting the most popular side thread over time",
    "text": "Plotting the most popular side thread over time\nWe can also look at which side thread is the most popular in any given 30 day period3, shown here below3 To be precise, we’ll be looking at which of the current top 25 threads is the most popular at any time. So if a thread was popular once, but has since dropped out of the top 25 you won’t find it here.\n\n\nCode to plot the most popular side thread\nwindow = \"30d\"\none_hot = resampled.rolling(window).sum().idxmax(axis=1)\nmode = pd.get_dummies(one_hot).rename(known_threads.to_dict()[\"thread_name\"], axis=1)\nmode = mode[[x for x in order if x in mode.columns]]\nlabels = {\"timestamp\": \"Date\", \"variable\": \"Side thread\"}\npalette = sns.color_palette(\"colorblind\", len(mode.columns))\ncolors = [f\"rgb{tuple(256*np.array(x))}\" for x in palette]\n\nfig = go.Figure()\nfor column, color in zip(mode.columns, colors):\n    fig.add_trace(\n        go.Scatter(\n            x=mode.index,\n            y=mode[column],\n            fill=\"tozeroy\",\n            mode=\"none\",\n            fillcolor=color,\n            name=column,\n            hoveron=\"points+fills\",\n        )\n    )\nfig.update_yaxes(range=[0, 1], visible=False, showticklabels=False)\nfig.show()\n\n\n\n                                                \n\n\nWhat I’d most like to draw your attention to with this plot is the four month stretch in 2021 when ternary was our most popular thread, and 25k counts were made in it. This is the only time ternary has ever been the most popular side thread, and the reason for the rapid rise is that somebody had decided to push the thread hard to reach an extra digit. Once that was accomplished, the activity declined to basically where it was before.\nI was originally going to include some deeper analysis of tug of war in this post as well, but cleaning up that data is going to take a lot longer than I thought, so that’s all for now! If you have any suggestions for things you’d like to see me do with the side thread data, let me know!"
  }
]