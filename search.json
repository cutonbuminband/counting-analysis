[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my website for publishing interesting bits and pieces of r/counting data and graphs in a central place. The code builds on the rcounting tools that I develop here"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "Imports and initialization\nWe start with some imports, which aren’t particularly interesting, so they’ve been folded.\n\n\nCode for importing packages\nimport pandas as pd\nfrom rcounting import side_threads, thread_navigation as tn\n\n\n\n\nValidating side threads\nThe rcounting tools have various pieces of functionality to validate the side threads it knows about, both in terms of whether the counts follow the expected format, and whether the thread obeys any special rules that might apply. Here’s an example of what that looks like in a python script.\n\n\nCode\n# Pick a comment early in a chain\ncomments = pd.DataFrame(tn.fetch_comments(\"gja0ehe\"))\nside_thread = side_threads.get_side_thread('slowestest')\nprint(\"The thread is valid!\" if side_thread.is_valid_thread(comments) else \"The thread is invalid\")\n\n\nThe thread is valid!\n\n\nThe thread is valid - excellent! There’s also a script that you can run from the command line to validate the most common threads. Try typing rcounting validate -h in a terminal to see how to use it.\n\n\nNetwork analysis\nThe rcounting tools also have functionality to do some network analysis. The following snippet will generate the (comment, replying to, weight) graph for the top 250 counters. The heavy lifting is done by the response graph [[file:../rcounting/analysis.py::def response_graph(df, n=250, username_column=“username”):][response_graph]] function in analysis.py.\n\n\nCode\nfrom rcounting import analysis, counters\nimport sqlite3\nfrom pathlib import Path\ndata_directory = Path(\"../data\")\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\ncounts = pd.read_sql(\"select comments.username \"\n                       \"from comments join submissions \"\n                       \"on comments.submission_id = submissions.submission_id \"\n                       \"where comments.position > 0 \"\n                       \"order by submissions.timestamp, comments.position\", db)\ncounts[\"username\"] = counts[\"username\"].apply(counters.apply_alias)\nn = 250\ngraph = analysis.response_graph(counts, n, username_column=\"username\")\ngraph.sort_values(ascending=False, by=\"Weight\").head()\n\n\n\n\n\n\n  \n    \n      \n      Source\n      Target\n      Weight\n    \n  \n  \n    \n      14949\n      thephilsblogbar2\n      GarlicoinAccount\n      98844\n    \n    \n      2862\n      GarlicoinAccount\n      thephilsblogbar2\n      98821\n    \n    \n      1650\n      Countletics\n      nonsensy\n      92922\n    \n    \n      12821\n      nonsensy\n      Countletics\n      92922\n    \n    \n      1563\n      Countletics\n      Antichess\n      72491"
  },
  {
    "objectID": "separators.html",
    "href": "separators.html",
    "title": "The use of separators",
    "section": "",
    "text": "We have access to the body of each comment, so it’s possible to do some of analysis on those. One interesting thing could be to look at whether a given count is comma separated, space separated or uses no separator at all. And a natural question to ask is how the distribution between those three types has changed over time\nSpecifically, we’ll define the three types of count as:\n\nComma separated counts look like [digit]*{1-3}(,[digit]*3)*\nSpace separated counts are the same, with the comma replaced by a space\nNo separated counts are defined as one of\n\nCounts with only one digit\nCounts with no separators between their first and last digit, with separators defined fairly broadly.\n\n\n\n\nCode for importing packages and loading data\nfrom pathlib import Path\nimport pandas as pd\nimport re\nimport sqlite3\nimport matplotlib.pyplot as plt\n\nfrom rcounting import parsing\nimport seaborn as sns\nsns.set_theme()\nfrom IPython.display import Markdown\ndata_directory = Path(\"../data\")\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\n\ncounts = pd.read_sql(\"select comments.body, comments.timestamp from comments join submissions \"\n                       \"on comments.submission_id = submissions.submission_id where comments.position > 0 \"\n                       \"order by submissions.timestamp, comments.position\", db)\ncounts['date'] = pd.to_datetime(counts['timestamp'], unit='s')\ncounts.drop('timestamp', inplace=True, axis=1)\n\n\nWe started by making the necessary imports and loading all the data; with that out of the way we can implement the rules defined above\n\n\nCode\ndata = counts.set_index('date')\n\ndata['body'] = data['body'].apply(parsing.strip_markdown_links)\ncomma_regex = re.compile(r'\\d{1,3}(?:,\\d{3})+')\ndata['is_comma_separated'] = data['body'].apply(lambda x: bool(re.search(comma_regex, x)))\nspace_regex = re.compile(r'\\d{1,3}(?: \\d{3})+')\ndata['is_space_separated'] = data['body'].apply(lambda x: bool(re.search(space_regex, x)))\ndef no_separators(body):\n    body = body.split('\\n')[0]\n    separators = re.escape(\"' , .*/\")\n    regex = (rf\"(?:^[^\\d]*\\d[^\\d]*$)|\"\n             rf\"(?:^[^\\d]*\\d[^{separators}]*\\d[^\\d]*$)\")\n    regex = re.compile(regex)\n    result = re.search(regex, body)\n    return bool(result)\n\ndata['no_separators'] = data['body'].apply(no_separators)\ndata.sort_index(inplace=True)\n\n\nOnce we have the data, we can get a 14-day rolling average, and resample the points to nice 6h intervals. The resampling makes plotting with pandas look nicer, since it can more easily deal with the x-axis.\n\n\nCode\nresampled = (data[['is_comma_separated', 'is_space_separated', 'no_separators']].rolling('14d').mean().resample('6h').mean() * 100)\nfig, ax = plt.subplots(1)\nresampled.plot(ax=ax, ylabel='Percentage of counts', lw=2)\nh, l = ax.get_legend_handles_labels()\nax.legend(h[:3],[\"commas\", \"spaces\", \"no separator\"])\nax.set_ylim([0, 100])\nax.set_xlabel('')\nplt.show()\n\n\n\n\n\nFigure 1: The separators used on r/counting over time\n\n\n\n\nThe result is shown on figure Figure 1\nNotice you can clearly see when the count crossed 100k: that’s when the ‘no separators’ line quickly drops from being the majority to being a clear minority of counts. That was followed by the era of commas, when the default format was just to use commas as separators. Over the last years, commas have significantly declined, and have now been overtaken by spaces as the most popular separator, although there’s a lot of variation depending on who exactly is active. No separators has bouts of activity, but is generally below the other two options. Pretty neat!"
  },
  {
    "objectID": "runs.html",
    "href": "runs.html",
    "title": "Longest runs",
    "section": "",
    "text": "Most counts on r/counting are made by two counters collaborating; the signature of this is that the n^th count and the (n + 2)^th count almost always have the same author. Usually we only consider such collaborative counting a run when the two counters reply to each other at a fairly rapid clip, but I’ve ignored that here\nFirstly, I’ve looked at the longest runs by total time, which I’ve defined as the longest periods of time when only two people counted in main. Unfortunately, that doesn’t give anything very interesting: these are all from early in the subreddit history, where hours would frequently pass between replies, and the runs I’ve found are generally less than ten counts long. An example is from the 24k counting thread, where just over 23 hours passed between two counts here here1. There was only one exception to this trend in the 2M era. Apparently we had a bit of a problem with spammers & farmers back then, and a really long chain of counts was deleted, and the count was continued from a valid point some hours later. The whole thing caused a bit of confusion.1 Plus that was a late chain which somehow became official. I guess we were less strict back then. \n\n\nCode\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport re\nimport sqlite3\nimport matplotlib.pyplot as plt\n\nfrom rcounting import side_threads, counters, analysis, thread_navigation as tn, parsing, units\nfrom rcounting.reddit_interface import reddit\nimport seaborn as sns\nsns.set_theme()\nfrom IPython.display import Markdown\ndata_directory = Path(\"../data/\")\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\ndf = pd.read_sql(\"select counters.canonical_username as username, timestamp, comment_id from comments join counters on comments.username=counters.username where comments.position > 0 order by timestamp\", db)\n\n\nLooking at the longest runs by total number of counts in the run is more interesting. so let’s do that. We take all our ordered counts and shift them by two; if the two authors are different, that means one streak has ended and a new one started. Taking the cumulative sum of all these changes means that each run is assigned the same number; exactly what we want. Table 1 shows the top ten runs in r/counting history, as well as their total length.2:2 I haven’t checked through all of them, but it’s likely all the lengths are off by one. That’s because I’ve forced every comment to be part of only one run, but the first comment in each run should simultaneously be the last comment in the previous run.\n\n\nCode\ncolumn = df['username']\nis_different_group = (column != column.shift(2))\ndf['group'] = is_different_group.cumsum()\ngroups = df.groupby('group')\nindices = (-groups.size().loc[groups.size() > 1]).sort_values(kind='mergesort').index\nold = groups.first().loc[indices]\nnew = groups.last().loc[indices]\nnew['username'] = groups.nth(1).loc[indices]['username']\nold['length'] = groups.size().loc[indices]\ncombined = old.join(new, rsuffix='2', lsuffix='1')\ncombined['dt'] = (combined['timestamp2'] - combined['timestamp1']) / units.HOUR\ncombined.drop(['timestamp1', 'timestamp2'], axis=1, inplace=True)\n\ndef link(comment):\n    body, submission = pd.read_sql(f\"select body, comment_id from comments where comment_id == '{comment}' order by timestamp desc limit 1\", db).iloc[0]\n    return f\"[{parsing.find_count_in_text(body):,}](/comments/{submission}/_/{comment})\"\n\nfinal = combined.head(10).reset_index(drop=True).copy()\n\nfinal[\"old_link\"] = final[\"comment_id1\"].apply(link)\nfinal[\"new_link\"] = final[\"comment_id2\"].apply(link)\nfinal.index += 1\ndef format_time(timedelta):\n    hours, rem = divmod(timedelta.total_seconds(), 3600)\n    minutes, seconds = divmod(rem, 60)\n    return f\"{int(hours):0>2}:{int(minutes):0>2}\"\nfinal['dt'] = pd.to_timedelta(final['dt'], unit='h').round('s').apply(format_time)\nMarkdown(final[['username1', 'username2', 'old_link', 'new_link', 'length', 'dt']].to_markdown(headers=['**Rank**', '**1st counter**', '**2nd counter**', '**Start**', '**End**', '**Length**', '**Duration**']))\n\n\n\n\nTable 1: The 10 longest runs in r/counting history\n\n\nRank\n1st counter\n2nd counter\nStart\nEnd\nLength\nDuration\n\n\n\n\n1\ndavidjl123\nCountletics\n3,037,002\n3,044,030\n7029\n02:44\n\n\n2\ndavidjl123\nCountletics\n3,190,001\n3,195,027\n5027\n02:11\n\n\n3\nCountletics\ndavidjl123\n3,101,028\n3,105,791\n4764\n01:54\n\n\n4\nCountletics\ndavidjl123\n3,072,012\n3,076,715\n4704\n01:56\n\n\n5\nCountletics\ndavidjl123\n3,201,005\n3,205,000\n3996\n01:41\n\n\n6\nCountletics\ndavidjl123\n3,090,222\n3,094,002\n3781\n01:45\n\n\n7\nCountletics\ndavidjl123\n3,064,318\n3,068,003\n3686\n01:37\n\n\n8\nCountletics\nLeMinerWithCheese\n4,569,377\n4,572,402\n3026\n02:43\n\n\n9\nAntichess\nCountletics\n4,218,058\n4,221,003\n2946\n01:31\n\n\n10\ndavidjl123\nGarlicoinAccount\n4,028,324\n4,031,002\n2679\n02:22\n\n\n\n\n\n\nThe first seven runs are by u/davidjl123 and u/Countletics in the early 3Ms, and all of the top ten involve either david or countletics. The first one which doesn’t either of them is number 17 between nonsensy and colby6666, starting at 3,456,003 and continuing for 2000 counts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysing rcounting data",
    "section": "",
    "text": "This page highlights some of the initial analysis I’ve done of the counts made on r/counting. To do this, I use the database of all counts that was put together by u/davidjl123 and u/Countletics (and others!), as modified by me. For more niche analysis, see some of the other pages on the sidebar on the left. Most of the figures and tables here have also been posted on the subreddit, but I wanted to have them in a central place. I also liked being able to show the code and the tables or figures it generates in the same document, so that people can see both. Some of it isn’t particularly interesting, so I’ve hidden it behind a code widget. You can unfold it just by clicking.\nThe idea of this page is also that I’ll try and keep the analysis current as more counts come in, while the other pages might slowly grow stale.\n\nImports and initialization\nWe’ll start with some imports, after which we can connect to the database of counts\n\n\nCode for importing packages and connecting to the database\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport re\nimport sqlite3\nimport matplotlib.pyplot as plt\n\nfrom rcounting import side_threads, counters, analysis, thread_navigation as tn, parsing\nfrom rcounting.reddit_interface import reddit\nimport seaborn as sns\nsns.set_theme()\nfrom IPython.display import Markdown\ndata_directory = Path(\"../data\")\n\ndb = sqlite3.connect(data_directory / \"counting.sqlite\")\n\n\n\n\nLoading Data\nThen we load some data, both the counts and the gets. We convert the timestamp to a date column, and add a “replying to” column, since some of what we’ll be doing later needs it.\n\n\nCode\ncounts = pd.read_sql(\"select comments.username, comments.submission_id, comments.timestamp \"\n                       \"from comments join submissions \"\n                       \"on comments.submission_id = submissions.submission_id \"\n                       \"where comments.position > 0 \"\n                       \"order by submissions.timestamp, comments.position\", db)\ncounts['date'] = pd.to_datetime(counts['timestamp'], unit='s')\ncounts[\"username\"] = counts[\"username\"].apply(counters.apply_alias)\ncounts.drop('timestamp', inplace=True, axis=1)\ncounts[\"replying_to\"] = counts[\"username\"].shift(1)\nprint(f\"There are {len(counts)} comments logged on main\")\ngets = counts.groupby(\"submission_id\").last().sort_values('date').reset_index()\ngets['basecount'] = (gets.index + 15) * 1000\ngets.loc[[0, 1], ['basecount']] = [0, 16691]\n\n\nThere are 4907935 comments logged on main\n\n\n\n\nCounting progress over time\nA first bit of analysis is to visualize the progress of r/counting over time. That isn’t particularly difficult to do\n\n\nCode\ndata = gets.set_index('date')\nax = (data.resample('30d')[[\"basecount\"]].mean() / 1e6).plot(ylabel='Current count [millions]', xlabel='Date')\nh, l = ax.get_legend_handles_labels()\nax.legend(h[:1],['count'])\nax.set_title('Counting progress over time')\nplt.show()\n\n\n\n\n\nFigure 1: Cumulative counts made on /r/counting as a function of time\n\n\n\n\nOn Figure 1 you can see that the counting rate varies quite a bit over time, with signifcant peaks and lulls in activity. Whether or not there are active runners really changes how fast the count is progressing!\n\n\nTotal counts vs k_parts\nWe can try plotting thread participation vs total counts. The expectation is that generally, people who’ve made more total counts will also have counted in more threads. However, some users might have periods where they make a count every now and then but never do any runs, leading to comparatively more k_parts. On the other hand, some counters might only do runs, giving a counts/thread of up to 500.\nWe’ll start by extracting the number of counts and the threads participated in, using the groupby functionality of pandas\n\n\nCode\n  groups = counts.groupby(\"username\")[\"submission_id\"]\n  k_parts = groups.nunique()\n  hoc = groups.count()\n  combined = pd.concat([k_parts, hoc], axis=1)\n  combined.columns = [\"k_parts\", \"total_counts\"]\n  combined = combined.query(\"k_parts >= 10\")\n\n\nWe can make a polynomial fit of this (well, a linear fit of the log-log quantities), and use matplotlib to plot that\n\n\nCode\nlinear_model = np.polyfit(np.log10(combined.k_parts), np.log10(combined.total_counts), 1)\nprint(linear_model)\naxis = np.linspace(1, combined.k_parts.max(), endpoint=True)\nfig, ax = plt.subplots(1, figsize=(8,5))\nax.scatter(combined.k_parts, combined.total_counts, alpha=0.7)\nax.plot(axis, 10**(np.poly1d(linear_model)(np.log10(axis))), linestyle=\"--\", color=\"0.3\",\n         lw=2)\nax.set_xlabel(\"Threads participated in \")\nax.set_ylabel(\"Total counts made\")\nax.set_yscale(\"log\")\nax.set_xscale(\"log\")\nax.set_xlim(left=10)\nax.set_ylim(bottom=10)\nplt.show()\n\n\n[1.3190586  0.72294786]\n\n\n\n\n\nFigure 2: The relationship between the total number of counts for each user, and then number of threads they’ve participated in\n\n\n\n\nYou can see what that looks like on Figure 2. The dashed line is a linear fit on the log-log plot, and it has a slope of 1.3. In this model, that means that if you double the total number of threads participated in by a user, you would expect to multiply their total counts by 2.5.\n\n\nNumber of partners and effective number of partners\nAs with the number of counts vs threads participated in, we can expect that different counters might have qualitatively different behaviour when it comes to how many counting partners they have, and how often they’ve counted with each one. Some counters might count a little bit with everybody, while others might run with only a few partners, and drop a count with others every now and then.\nTo quantify how uneven the counting distribution is we can look at the effective number of partners of each counter, and compare with the actual number of partners.\n\n\nCode\nsorted_counters = counts.groupby(\"username\").size().sort_values(ascending=False)\ntop_counters = [x for x in sorted_counters.index[:35] if not counters.is_banned_counter(x)][:30]\ntop = sorted_counters.filter(items=top_counters)\ndf = counts.loc[counts[\"username\"].isin(top_counters)].groupby([\"username\", \"replying_to\"]).size()\neffective_partners = df.groupby(level=0).apply(analysis.effective_number_of_counters).to_frame()\npartners = df.groupby(level=0).count()\ncombined = pd.concat([top, effective_partners, partners], axis=1)\ncombined[\"HOC rank\"] = range(1, len(combined) + 1)\ncombined.columns = [\"counts\", \"c_eff\", \"c\", \"rank\"]\ncombined = combined[[\"rank\", \"c\", \"c_eff\"]]\ncombined.c_eff = combined.c_eff.round().astype(int)\ncombined.columns = [\"HOC rank\", \"N\", \"N_(effective)\"]\ncombined.index.name = \"Username\"\ncombined.head(25)\n\n\n\n\n\n\n  \n    \n      \n      HOC rank\n      N\n      N_(effective)\n    \n    \n      Username\n      \n      \n      \n    \n  \n  \n    \n      thephilsblogbar2\n      1\n      1371\n      12\n    \n    \n      Countletics\n      2\n      507\n      8\n    \n    \n      davidjl123\n      3\n      1578\n      23\n    \n    \n      Antichess\n      4\n      756\n      9\n    \n    \n      GarlicoinAccount\n      5\n      388\n      4\n    \n    \n      Smartstocks\n      6\n      1054\n      29\n    \n    \n      nonsensy\n      7\n      261\n      3\n    \n    \n      TheNitromeFan\n      8\n      1879\n      33\n    \n    \n      atomicimploder\n      9\n      2441\n      31\n    \n    \n      qwertylool\n      10\n      371\n      7\n    \n    \n      Ezekiel134\n      11\n      894\n      13\n    \n    \n      Trial-Name\n      12\n      154\n      4\n    \n    \n      rideride\n      13\n      684\n      11\n    \n    \n      RandomRedditorWithNo\n      14\n      744\n      22\n    \n    \n      Urbul\n      15\n      1084\n      30\n    \n    \n      Mooraell\n      16\n      1193\n      24\n    \n    \n      qualw\n      17\n      267\n      10\n    \n    \n      kdiuro13\n      18\n      841\n      24\n    \n    \n      Removedpixel\n      19\n      1198\n      25\n    \n    \n      Adinida\n      20\n      299\n      9\n    \n    \n      ClockButTakeOutTheL\n      21\n      138\n      2\n    \n    \n      TehVulpez\n      22\n      674\n      19\n    \n    \n      rschaosid\n      23\n      335\n      18\n    \n    \n      LeMinerWithCheese\n      24\n      89\n      7\n    \n    \n      timo78888\n      25\n      320\n      15\n    \n  \n\n\n\n\nWe can also get the replying-to and replied-by stats for a single user\n\n\nCode\ncounter = \"thephilsblogbar2\"\nnick = \"phil\"\nsubset = counts.loc[counts[\"username\"] == counter].copy()\nreplied_by = counts['username'].shift(-1).loc[subset.index]\nsubset['replied_by'] = replied_by\nresult = pd.concat([subset.groupby(\"replied_by\").count().iloc[:, 0].sort_values(ascending=False),\n                    subset.groupby(\"replying_to\").count().iloc[:, 0].sort_values(ascending=False)], axis=1).head(10)\nMarkdown(result.to_markdown(headers=['Counting partner', f'No. of replies by {nick}', f'No. of replies to {nick}']))\n\n\n\n\nTable 1: The most popular counting partners of a single user\n\n\n\n\n\n\n\nCounting partner\nNo. of replies by phil\nNo. of replies to phil\n\n\n\n\nGarlicoinAccount\n98821\n98844\n\n\nCountletics\n34364\n34433\n\n\nClockButTakeOutTheL\n34200\n34192\n\n\nTheNitromeFan\n16095\n16193\n\n\nAntichess\n15198\n15329\n\n\nTrial-Name\n11529\n11560\n\n\namazingpikachu_38\n11492\n11483\n\n\nCutOnBumInBandHere9\n11315\n11420\n\n\natomicimploder\n10692\n10760\n\n\nnonsensy\n10535\n10513\n\n\n\n\n\n\n\n\nOldest counters\nWe can see who the oldest still-active counters are, where still-active is generously defined as “having made a count within the last six months”.\n\n\nCode\ncutoff_date = pd.to_datetime('today') - pd.Timedelta('180d')\nactive_counters = counts.loc[counts['date'] > cutoff_date].groupby(\"username\").groups.keys()\noldest_counters = counts.loc[counts['username'].isin(active_counters)].groupby(\"username\")[\"date\"].agg([min, max])\noldest_counters = oldest_counters.sort_values('min').head(25)\nMarkdown(oldest_counters.apply(lambda x: x.dt.date).to_markdown(headers=[\"**username**\", \"**First Count**\", \"**Latest Count**\"]))\n\n\n\n\nTable 2: The 25 currently-active counters who’ve been counting for the longest time\n\n\nusername\nFirst Count\nLatest Count\n\n\n\n\nZ3F\n2012-06-10\n2022-12-25\n\n\nCanGreenBeret\n2012-06-12\n2022-12-06\n\n\n949paintball\n2012-06-15\n2022-12-27\n\n\nPookah\n2012-09-06\n2022-11-25\n\n\nkdiuro13\n2013-04-07\n2022-10-28\n\n\nzhige\n2013-05-03\n2022-12-25\n\n\nfalsehood\n2013-06-24\n2022-12-24\n\n\nAschebescher\n2013-06-24\n2022-11-08\n\n\nsudofox\n2013-12-13\n2022-09-16\n\n\nCoStCo19\n2013-12-13\n2022-07-18\n\n\nCutOnBumInBandHere9\n2013-12-13\n2023-01-06\n\n\noohbopbadoo\n2013-12-13\n2022-10-11\n\n\nKingCaspianX\n2014-01-18\n2022-09-28\n\n\nOreoObserver\n2014-01-25\n2023-01-04\n\n\nyeontura\n2014-02-26\n2022-10-25\n\n\natomicimploder\n2014-02-26\n2023-01-09\n\n\norigamimissile\n2014-03-10\n2022-10-27\n\n\nSThor\n2014-03-10\n2022-08-04\n\n\nrideride\n2014-04-07\n2022-10-05\n\n\nJuqu\n2014-04-12\n2023-01-08\n\n\nGregsquatch\n2014-04-27\n2022-09-24\n\n\nrschaosid\n2014-04-27\n2022-09-28\n\n\nbittrashed\n2014-05-02\n2022-10-07\n\n\nNoBreadsticks\n2014-05-15\n2022-08-13\n\n\nBlimp_Blimp\n2014-06-29\n2022-12-13\n\n\n\n\n\n\n\n\nGets and streaks\nSimilarly to the oldest counters, we can see what the longest difference between a counter’s first and last get is, and that’s shown on Table 3. Some counters have been active and getting gets for quite a while!\n\n\nCode\n  Markdown(gets.groupby('username').agg(lambda x: x.index[-1] - x.index[0]).iloc[:, 0].sort_values(ascending=False).head(10).to_markdown(headers=[\"**Username**\", \"**Get span**\"]))\n\n\n\n\nTable 3: The longest differences between the first and last get of r/counting users (1000s of counts)\n\n\nUsername\nGet span\n\n\n\n\natomicimploder\n4740\n\n\norigamimissile\n4577\n\n\nManiac_34\n4575\n\n\nMooraell\n4563\n\n\nmusicbuilder\n4515\n\n\nPookah\n4500\n\n\nFartyMcNarty\n4373\n\n\nTheNitromeFan\n4305\n\n\ndavidjl123\n4111\n\n\nSmartstocks\n4083\n\n\n\n\n\n\nWe can also calculate what the longest get streaks are.\n\n\nCode\ny = gets['username']\ngroups = gets.groupby((y != y.shift()).cumsum())\ncolumns = ['username', 'submission_id', 'basecount']\nlength = 10\n\nindices = (-groups.size()).sort_values(kind='mergesort').index\nold = groups.first().loc[indices, columns]\nnew = groups.last().loc[indices, columns]\ncombined = old.join(new, rsuffix='_new')\ncombined = combined.loc[~combined['username'].apply(counters.is_banned_counter)].head(length).reset_index(drop=True)\ncombined['old_link'] = combined.apply(lambda x: f'[{int(x.basecount / 1000) + 1}K](https://reddit.com/comments/{x.submission_id}/)', axis=1)\ncombined['new_link'] = combined.apply(lambda x: f'[{int(x.basecount_new / 1000) + 1}K](https://reddit.com/comments/{x.submission_id_new}/)', axis=1)\ncombined['streak'] = 1 + (combined['basecount_new'] - combined['basecount']) // 1000\ncombined.index += 1\ncombined.index.name = \"Rank\"\nMarkdown(combined[['username', 'old_link', 'new_link', 'streak']].to_markdown(headers=['**Rank**', '**username**', '**First Get**', '**Last Get**', '**Streak Length**']))\n\n\n\n\nTable 4: The longest streak\n\n\nRank\nusername\nFirst Get\nLast Get\nStreak Length\n\n\n\n\n1\nCountletics\n2896K\n2914K\n19\n\n\n2\nLeMinerWithCheese\n4570K\n4584K\n15\n\n\n3\ndavidjl123\n3038K\n3047K\n10\n\n\n4\nCountletics\n3091K\n3100K\n10\n\n\n5\nCountletics\n3190K\n3199K\n10\n\n\n6\nCountletics\n4384K\n4393K\n10\n\n\n7\nqualw\n2042K\n2049K\n8\n\n\n8\ndavidjl123\n3726K\n3733K\n8\n\n\n9\nqwertylool\n4440K\n4447K\n8\n\n\n10\ncountmeister\n290K\n296K\n7\n\n\n\n\n\n\nThe core of the extraction is the line that says groups = gets.groupby((y != y.shift()).cumsum()). Let’s unpack it:\n\ny != y.shift() assigns a value of True to all threads with a username that’s different from their predecessor\n.cumsum() sums up all these True values. The net result is that each get streak is given its own unique number\n.groupby() extracts these groups for later use\n\nThe groups are then sorted according to size, and the largest ones are shown in Table 4"
  }
]