#+PROPERTY: header-args:jupyter-python  :session dst :kernel reddit
#+PROPERTY: header-args    :pandoc t :tangle yes
#+TITLE: Daylight Saving Time: On modelling and robustness

In a [[https://cutonbuminband.github.io/counting-analysis/time.html][previous post]] I looked at the daily rhythm of [[http://www.reddit.com/r/counting][r/counting]], at what time of day the subreddit is most active, and how that has changed throughout the years. Zooming in on a small aspect, this post will focus on the effect of daylight saving time on the timing of counts.

Now, DST occurs at different times (if at all) throughout the world, so I've focussed exclusively on DST in the US & Canada, where it starts on the second Sunday in March and ends on the first Sunday of November of each year[fn:1][fn:2].

I have the UTC timestamps for every count, so it's possible to compare our counting activity just before DST comes into force with our counting activity just afterwards, and see whether there's a difference. If counts always follow the same pattern in local time, and all counters observe DST at the same time, then that should show up as a rigid shift in the data. 

* Imports and loading
We'll start off with some code to import the relevant packages and load the data.

#+begin_src jupyter-python
  #| code-fold: true
  #| code-summary: "Code for importing packages and loading data"
  from pathlib import Path
  import pandas as pd
  import sqlite3
  import matplotlib.pyplot as plt
  import numpy as np
  from rcounting import counters, analysis, graph_tools, plots, units
  from IPython.display import Markdown
  import itertools
  import seaborn as sns
  from datetime import datetime, timedelta
  sns.set_theme()

  data_directory = Path("../../data")
  db = sqlite3.connect(data_directory / "counting.sqlite")
#+end_src

* A first model

To see the effect of dst, we can compare the time of day plots for the weeks just before and just after DST is introduced, and see if there are any obvious differences. To maximise the effect of DST, it makes sense to focus only on counts that occurred Monday to Friday, since people's school or working hours should generally be more regular than their weekends.


To start with, we need some code to find when DST started on any given year
#+begin_src jupyter-python
  #| code-fold: true
  #| code-summary: "Finding the start of DST for every year"
  days = ["monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday"]
  def find_nth_weekday(year, month, weekday, n):
      d = datetime(year, month, 1 + 7 * (n - 1))
      offset = (days.index(weekday.lower()) - d.weekday()) % 7
      return d + timedelta(offset) + timedelta(hours=2)

  WEEK = 7 * 24 * units.HOUR
  OFFSET = 5 * units.HOUR
  BIN_WIDTH = 30
  nbins = int(units.DAY / BIN_WIDTH)
  minval, maxval = -3, 3
  BIN_TO_MINUTE = (BIN_WIDTH / 60)
  x = np.linspace(0, units.DAY, nbins, endpoint=False) + BIN_WIDTH / 2
  dy = 0.07
  years = range(2012, 2024)
  import calendar
  dst_start = {year: calendar.timegm(find_nth_weekday(year, 3, 'sunday', 2).timetuple()) for year in years}
#+end_src

With that out of the way, we'll select the weeks just before the start of DST and the weeks just after the end of DST for every year in r/counting's history.

#+begin_src jupyter-python
  #| code-summary: "Code for getting the raw data into slightly more manageable shape."
  def wrangle(data, week_map=dst_start):
      data = data.reset_index(drop=True)
      data["timestamp"] = data["timestamp"] - OFFSET
      data["date"] = pd.to_datetime(data["timestamp"], unit="s")
      data['time'] = (data['timestamp']) % units.DAY
      data["week"] = (np.floor((data["timestamp"]
                                - pd.to_datetime(data["timestamp"], unit="s").dt.year.map(week_map))
                               / WEEK)
                      .astype(int))
      data["year"] = data["date"].dt.year
      return data

  def generate_kdes(df):
      n_weeks = len(df["week"].unique())
      kdes = (df
              .groupby("week")["time"]
              .apply(lambda x: pd.Series(analysis.fft_kde(
                  x,
                  nbins,
                  kernel="normal_distribution",
                  sigma=0.02)[1] * nbins))
              .reset_index()[["week", "time"]])
      kdes["x"] = np.hstack(n_weeks*[x])
      kdes.columns = ["week", "rate", "time"]
      return kdes

#+end_src

#+begin_src jupyter-python
  query = f"select timestamp, username from comments where timestamp between {{}} and {{}} order by timestamp"

  spring_all = wrangle(
      pd.concat(
          [
              pd.read_sql(query.format(x + OFFSET - minval * WEEK, x + OFFSET + maxval * WEEK), db)
              for x in dst_start.values()
          ]
      )
  )

  def mask(df):
      return ((df["date"].dt.weekday < 5)
              & (-2 <= df["week"])
              & (df["week"] < 2))

  spring = spring_all[mask(spring_all)].copy()
  spring_kdes = generate_kdes(spring)
  week_map = {-2: "Control without DST",
              -1: "Without DST",
              0: "With DST",
              1: "Control with DST"}
  spring_kdes["week_name"] = spring_kdes["week"].map(week_map)

#+end_src

And we're ready to plot the distribution of counts throughout the day for the week before and the week after the introduction of DST, and see how they differ
#+begin_src jupyter-python
  for week in ["With DST", "Without DST"]:
      df = spring_kdes.query("week_name == @week")
      plt.fill_between(df["time"], df["rate"], alpha=0.8, label=week)
  ax = plt.gca()
  plots.make_time_axis(ax)
  ax.set_ylabel("Counting rate (arbitrary units)")
  ax.set_xlabel("Time of Day (UTC - 5)")
  ax.legend()
#+end_src


The shape of the two plots is similar, and it looks like the plot with DST is generally leading the one without, as would be expected if one was just a rigid shift of the other. But it's certainly not a perfect match, and it's hard to see from the curves just how much the DST curve is leading.

We can try and see what the optimal shift of the DST curve would be to get it to match the curve without DST.
#+begin_src jupyter-python
  def calculate_shifted_overlap(df, week1, week2):
      fixed = df.loc[df["week_name"] == week2, "rate"].to_numpy()
      rotating = df.loc[df["week_name"] == week1, "rate"].to_numpy()
      norm = np.trapz(fixed * rotating, x=x)
      shifts = [np.trapz(fixed * np.roll(rotating, i), x=x) / norm for i in range(len(fixed))]
      optimal_shift = (np.argmax(shifts) + nbins / 2) % nbins - nbins/2
      return shifts, optimal_shift

  shifts, optimal_shift = calculate_shifted_overlap(spring_kdes, "With DST","Without DST")

  plt.plot(shifts)
  plt.xlim(0, len(shifts))
  ax = plt.gca()
  ticks, labels = zip(*[(x * 120, f"{x:02d}:00") for x in range(0, 25, 3)])
  ax.set_xticks(ticks)
  ax.set_xticklabels(labels)
  print(f"The optimal shift is {int(optimal_shift * BIN_TO_MINUTE)} minutes.")

  ax.set_xlabel("Shift (hours)")
  ax.set_ylabel("Similarity score")
  plt.show()
#+end_src

That's a bit less than one hour, but it's still suggestive. Apparently we can use the counting data to determine whether or not DST is currently active.

So, case closed, right?
* Validating the model
Not so fast.

It could be that there's a shift of one hour every week and DST has nothing to do with it! More seriously, there are other changes happening throughout the time period apart from DST; in the spring the days are getting longer, particularly the evenings, and maybe that's what's driving the change. And I haven't at all looked at what happens when the clocks go back.

** Adding more weeks
Let's start by looking at what happens before DST is active. For the preceding analysis to be valid, we'd need the distribution of counts throughout the day to be basically the same for the period just before DST is active and the control period one week before that.
#+begin_src jupyter-python
  for week in ["Without DST", "Control without DST"]:
      df = spring_kdes.query("week_name == @week")
      plt.fill_between(df["time"], df["rate"], alpha=0.8, label=week)
  ax = plt.gca()
  plots.make_time_axis(ax)
  ax.set_ylabel("Counting rate (arbitrary units)")
  ax.set_xlabel("Time of Day (UTC - 5)")
  ax.legend()
#+end_src

Hm. Those two curves might be slightly more aligned than the two with and without DST, but it's not super clear. We can check the optimal shift

#+begin_src jupyter-python
  _, optimal_shift = calculate_shifted_overlap(spring_kdes, "Without DST", "Control without DST")
  print(f"The optimal shift is {int(optimal_shift * BIN_TO_MINUTE)} minutes.")
#+end_src

That's an even bigger shift than the one that happened when DST was introduced! We can plot four the curves for the two weeks before and after DST together and see if there's any obvious pattern.
#+begin_src jupyter-python
  spring_kdes["shifted_rate"] = spring_kdes["rate"] + (spring_kdes["week"] + 2) * dy
  ax = sns.lineplot(spring_kdes, x="time", y="shifted_rate", hue="week_name")
  ax.legend_.set_title("Week")
  plots.make_time_axis(ax)
  ax.legend(loc="upper center", ncol=2)
  ax.set_ylabel("Counting rate (arbitrary units)")
  ax.set_xlabel("Time of Day (UTC - 5)")
  ax.set_ylim(0, 0.34)
#+end_src

If you didn't have the legend, would you be able to tell which two of these curves were with DST and which were without?

** Including the end of DST
We can try and see if including the data for when the clocks go back each year makes any difference
#+begin_src jupyter-python
  dst_end = {year: calendar.timegm(find_nth_weekday(year, 11, 'sunday', 1).timetuple()) for year in years}
  autumn_all = wrangle(
      pd.concat(
          [
              pd.read_sql(query.format(x + OFFSET - minval * WEEK, x + OFFSET + maxval * WEEK), db)
              for x in dst_end.values()
          ]
      ),
      dst_end
  )

  autumn = autumn_all[mask(autumn_all)].copy()
  autumn["week"] = -1 - autumn_all["week"]

#+end_src

#+begin_src jupyter-python
  #| label: fig-autumn-kdes
  #| fig-cap: The aggregated activity on r/counting in the two weeks leading up to the end of DST, and the two weeks after it.
  kdes = generate_kdes(pd.concat([spring, autumn]))
  kdes["week_name"] = kdes["week"].map(week_map)
  kdes["shifted_rate"] = kdes["rate"] + (kdes["week"] + 2) * dy
  ax = sns.lineplot(kdes, x="time", y="shifted_rate", hue="week_name")
  ax.legend_.set_title("Week")
  plots.make_time_axis(ax)
  ax.legend(loc="upper center", ncol=2)
  ax.set_ylabel("Counting rate (arbitrary units)")
  ax.set_xlabel("Time of Day (UTC - 5)")
  ax.set_ylim(0, 0.34)
  _, optimal_shift = calculate_shifted_overlap(kdes, "With DST", "Without DST")
  print(f"The optimal shift is {int(optimal_shift * BIN_TO_MINUTE)} minutes.")

#+end_src

As before -- would you be able to tell which of these graphs were with DST and which were without if you didn't have the legend?

** Summing up

The validation of the model has revealed that the activity on r/counting varies enough on a week to week basis that our initial assumptions are incorrect, and we can't just treat the activity as a constant background with a DST signal on top. If we want to see the effect of DST, we're going to have to come up with something more clever.

* More Advanced Models

** Disaggregating the years
What we did in the previous section was to aggregate the activity on r/counting across all the years it's been active. After that, we honed in on specific weeks near the time of year when the clocks change, and asked if we could see a shift in the data.

We've seen that the activity on r/counting isn't stable over time, so maybe we're losing information by aggregating all the years, and the signal would be clearer if we looked at each year separately.

Before we can make the comparison we're going to need a way of boiling down the information. As we saw on @fig-autumn-kdes and friends in the previous section, spotting the shift by eye is very difficult, and if we further split the plot into a new line for each year, it's going to become completely unreadable.

We need a way of compressing each (week, year) pair to a single point: that way whatever plot we end up producing should hopefully still be legible.

If we're willing to use the fact that we know the DST offset is one hour, we can reuse a lot of what we did in the previous section: For each week, we can calculate how much the distribution resembles that of the week before, and we can also calculate how much the distribution resembles the 1 hour /shifted/ distribution from the week before.

For most of the year, it should be the case that the unshifted distribution is more similar then the shifted distribution. But, for the week where the clocks change, the shifted distribution should be more similar. So, we can calculate the similarity of the lagged and shifted distribution, and subtract the similarity of just the lagged distribution, and we've arrived at our DST fingerprint. For most weeks, it should give a negative value, but for the week where the clocks change (and maybe a bit afterwards if people are slow to adapt), it should give a positive value, before dropping back down to the negatives.

Let's see how it goes!

#+begin_src jupyter-python
  def dst_fingerprint(df, period="spring"):
      """Calculate the dst fingerprint for a single year"""
      transitions = dst_start if period == "spring" else dst_end
      x = df.resample("300s", on="date").size()
      rates = x.div(x.groupby(pd.Grouper(freq="1d")).transform("sum")).to_frame(name="rate")
      rates["year"] = rates.index.year
      rates.index = rates.index - pd.to_datetime(rates.index.year.map(transitions), unit="s")
      shifted = rates.shift(freq="7d")
      shift = "-1h" if period == "spring" else "1h"
      dst_shifted = shifted.shift(freq=shift).reset_index().set_index("date")

      dfs = []

      for df in [shifted, dst_shifted]:
          f1 = pd.merge(rates, df, left_index=True, right_index=True)
          f1["delta"] = (f1["rate_x"] - f1["rate_y"])**2
          dfs.append(f1.groupby(f1.index.days // 7)["delta"].sum())


      return dfs[1] - dfs[0]

  def multiple_dst_fingerprints(df, period="spring"):
      groups = df.groupby("year").apply(dst_fingerprint, period=period)
      return groups.reset_index().melt(id_vars="year")

  df = multiple_dst_fingerprints(spring_all)
  ax = sns.relplot(df, x="date", y="value", hue="year", palette="plasma").ax
  ax.axhline(0, color="0.5", linestyle="--")
  ax.set_xlabel("Weeks after start of DST")
  ax.set_ylabel("DST fingerprint")
#+end_src

Hm. This isn't very promising. The DST signal should show up in this plot in the fact that the points at 0 should lie significantly higher than all the others. That's not really the case at all.

We can do the same thing for when DST ends, just for good measure, to see if the signal shows up there:

#+begin_src jupyter-python
  df = multiple_dst_fingerprints(autumn_all, "autumn")
  ax = sns.relplot(df, x="date", y="value", hue="year", palette="plasma").ax
  ax.axhline(0, color="0.5", linestyle="--")
  ax.set_xlabel("Weeks after end of DST")
  ax.set_ylabel("DST fingerprint")
#+end_src

Unfortunately, we didn't have any luck there either. Before giving up completely and abandoning this as a fool's errand, there's one or two more things we can try.

** Disaggregating the different counters

Regulars of r/counting will know that it's not the same people who count every week, as evidenced by the fact that the top fifteen counters list helpfully provided in each FTF isn't just a repeat from week to week. Perhaps this is one cause of the lack of pattern in the counting times. It's certainly possible to imagine a world where counters are perfectly regular, but the different schedules of different counters coupled with their different activity from week to week adds up to a huge mess.

So we can keep going with the disaggregation, and see if we get a clearer signal when we compare the activity of individual counters from week to week. 


Instead of calculating the DST fingerprint described in the previous section, we'll be looking just at the week consistency score, which is just how similar each week is to the preceding one. This is slightly easier to work with[fn:3], and should show much the same thing. The only difference is that the sign of DST will be a dip at 0, instead of a peak.
#+begin_src jupyter-python
   def similarity_score(df):
       kdes = generate_kdes(df)
       groups = kdes.groupby("week")["rate"]
       norm = groups.transform(np.linalg.norm)
       kdes["rate"] /= norm
       overlaps = ((kdes.
                    set_index(["week", "time"])
                    .groupby("time")["rate"]
                    .diff() ** 2)
                   .groupby(level=0)
                   .sum())
       return 1 - overlaps / 2
#+end_src

#+begin_src jupyter-python
  scores = spring_all.groupby(["year", "username"]).apply(similarity_score)
  scores.name = "fingerprint"
  scores[scores == 1] = 0
  sizes = (spring_all.groupby(["year", "username"]).size()
           / len(spring_all))
  similarity = scores.reset_index(level=2)
  similarity["similarity"] *= sizes
  similarity = similarity[similarity["similarity"] != 0]
  ax=sns.relplot(data=(similarity
                       .groupby(["week"])["similarity"]
                       .sum()
                       .reset_index()),
                 x="week",
                 y="similarity").ax
                 # hue="year",
                 # palette="plasma").ax
  ax.set_xlabel("Weeks after start of DST")
  ax.set_ylabel("Week consistency score")
#+end_src

** Looking only at the most regular counters
- Strategy:
  - Find the most regular counter(s) four weeks before the start of DST each year
  - Sanity check that they probably are located in the US
  - And then use the method just to describe to see if there's a difference around DST
  - Selecting on a different period than the one we're testing is important, because reasons.

#+begin_src jupyter-python
  counters = (scores[scores != 1]
              .reset_index()
              .query("week== -2")
              .sort_values(["year", "similarity"], ascending=False)
              .groupby("year")
              .head(5)
              .set_index(["year", "username"])
              .index)
  subset = spring_all.set_index(["year", "username"]).loc[counters]
  scores = subset.groupby(["year", "username"]).apply(similarity_score)
  scores.name = "similarity"
  scores[scores == 1] = 0
  sizes = subset.groupby(["year", "username"]).size() / len(subset)
  similarity = scores.reset_index(level=2)
  similarity["similarity"] *= sizes
  similarity = similarity[similarity["similarity"] != 0]
  ax=sns.relplot(data=(similarity
                       .groupby(["week"])["similarity"]
                       .sum()
                       .reset_index()),
                 x="week",
                 y="similarity",).ax
                 # hue="year",
                 # palette="plasma").ax
  ax.set_xlabel("Weeks after start of DST")
  ax.set_ylabel("Week consistency score")
#+end_src




* Conclusion

If you want to find out whether or not the US currently has DST, then looking at the comments on r/counting is not a viable method for doing so. I would suggest just googling it instead.

This post ended up being much longer than expected (and a fair bit longer than the reddit comment that it's based on), mainly because I've had to change the conclusion along the way.

In the original, and in my first draft, I wasn't as thorough with my robustness analysis as I've been here. That meant that i was more convinced by the hints of a DST signal in the data, and the conclusion reflected that. Unfortunately, this post has demonstrated that it just isn't there. On the positive side, the post has also demonstrated the value of checking assumptions, validating any model that you might come up with, and generally having a healthy dose of skepticism towards any new discoveries -- especially your own.

And that's perhaps as good a place as any to end.

Until next time!


[fn:1]Apart from Hawaii and Arizona, which are weird
[fn:2]That hasn't always been the DST rule, but it's been the case for as long as r/c has existed
[fn:3]In that it involves slightly less fiddling about with indices
